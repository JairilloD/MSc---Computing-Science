\documentclass[11pt]{article}

    \usepackage[breakable]{tcolorbox}
    \usepackage{parskip} % Stop auto-indenting (to mimic markdown behaviour)
    

    % Basic figure setup, for now with no caption control since it's done
    % automatically by Pandoc (which extracts ![](path) syntax from Markdown).
    \usepackage{graphicx}
    % Keep aspect ratio if custom image width or height is specified
    \setkeys{Gin}{keepaspectratio}
    % Maintain compatibility with old templates. Remove in nbconvert 6.0
    \let\Oldincludegraphics\includegraphics
    % Ensure that by default, figures have no caption (until we provide a
    % proper Figure object with a Caption API and a way to capture that
    % in the conversion process - todo).
    \usepackage{caption}
    \DeclareCaptionFormat{nocaption}{}
    \captionsetup{format=nocaption,aboveskip=0pt,belowskip=0pt}

    \usepackage{float}
    \floatplacement{figure}{H} % forces figures to be placed at the correct location
    \usepackage{xcolor} % Allow colors to be defined
    \usepackage{enumerate} % Needed for markdown enumerations to work
    \usepackage{geometry} % Used to adjust the document margins
    \usepackage{amsmath} % Equations
    \usepackage{amssymb} % Equations
    \usepackage{textcomp} % defines textquotesingle
    % Hack from http://tex.stackexchange.com/a/47451/13684:
    \AtBeginDocument{%
        \def\PYZsq{\textquotesingle}% Upright quotes in Pygmentized code
    }
    \usepackage{upquote} % Upright quotes for verbatim code
    \usepackage{eurosym} % defines \euro

    \usepackage{iftex}
    \ifPDFTeX
        \usepackage[T1]{fontenc}
        \IfFileExists{alphabeta.sty}{
              \usepackage{alphabeta}
          }{
              \usepackage[mathletters]{ucs}
              \usepackage[utf8x]{inputenc}
          }
    \else
        \usepackage{fontspec}
        \usepackage{unicode-math}
    \fi

    \usepackage{fancyvrb} % verbatim replacement that allows latex
    \usepackage{grffile} % extends the file name processing of package graphics
                         % to support a larger range
    \makeatletter % fix for old versions of grffile with XeLaTeX
    \@ifpackagelater{grffile}{2019/11/01}
    {
      % Do nothing on new versions
    }
    {
      \def\Gread@@xetex#1{%
        \IfFileExists{"\Gin@base".bb}%
        {\Gread@eps{\Gin@base.bb}}%
        {\Gread@@xetex@aux#1}%
      }
    }
    \makeatother
    \usepackage[Export]{adjustbox} % Used to constrain images to a maximum size
    \adjustboxset{max size={0.9\linewidth}{0.9\paperheight}}

    % The hyperref package gives us a pdf with properly built
    % internal navigation ('pdf bookmarks' for the table of contents,
    % internal cross-reference links, web links for URLs, etc.)
    \usepackage{hyperref}
    % The default LaTeX title has an obnoxious amount of whitespace. By default,
    % titling removes some of it. It also provides customization options.
    \usepackage{titling}
    \usepackage{longtable} % longtable support required by pandoc >1.10
    \usepackage{booktabs}  % table support for pandoc > 1.12.2
    \usepackage{array}     % table support for pandoc >= 2.11.3
    \usepackage{calc}      % table minipage width calculation for pandoc >= 2.11.1
    \usepackage[inline]{enumitem} % IRkernel/repr support (it uses the enumerate* environment)
    \usepackage[normalem]{ulem} % ulem is needed to support strikethroughs (\sout)
                                % normalem makes italics be italics, not underlines
    \usepackage{soul}      % strikethrough (\st) support for pandoc >= 3.0.0
    \usepackage{mathrsfs}
    

    
    % Colors for the hyperref package
    \definecolor{urlcolor}{rgb}{0,.145,.698}
    \definecolor{linkcolor}{rgb}{.71,0.21,0.01}
    \definecolor{citecolor}{rgb}{.12,.54,.11}

    % ANSI colors
    \definecolor{ansi-black}{HTML}{3E424D}
    \definecolor{ansi-black-intense}{HTML}{282C36}
    \definecolor{ansi-red}{HTML}{E75C58}
    \definecolor{ansi-red-intense}{HTML}{B22B31}
    \definecolor{ansi-green}{HTML}{00A250}
    \definecolor{ansi-green-intense}{HTML}{007427}
    \definecolor{ansi-yellow}{HTML}{DDB62B}
    \definecolor{ansi-yellow-intense}{HTML}{B27D12}
    \definecolor{ansi-blue}{HTML}{208FFB}
    \definecolor{ansi-blue-intense}{HTML}{0065CA}
    \definecolor{ansi-magenta}{HTML}{D160C4}
    \definecolor{ansi-magenta-intense}{HTML}{A03196}
    \definecolor{ansi-cyan}{HTML}{60C6C8}
    \definecolor{ansi-cyan-intense}{HTML}{258F8F}
    \definecolor{ansi-white}{HTML}{C5C1B4}
    \definecolor{ansi-white-intense}{HTML}{A1A6B2}
    \definecolor{ansi-default-inverse-fg}{HTML}{FFFFFF}
    \definecolor{ansi-default-inverse-bg}{HTML}{000000}

    % common color for the border for error outputs.
    \definecolor{outerrorbackground}{HTML}{FFDFDF}

    % commands and environments needed by pandoc snippets
    % extracted from the output of `pandoc -s`
    \providecommand{\tightlist}{%
      \setlength{\itemsep}{0pt}\setlength{\parskip}{0pt}}
    \DefineVerbatimEnvironment{Highlighting}{Verbatim}{commandchars=\\\{\}}
    % Add ',fontsize=\small' for more characters per line
    \newenvironment{Shaded}{}{}
    \newcommand{\KeywordTok}[1]{\textcolor[rgb]{0.00,0.44,0.13}{\textbf{{#1}}}}
    \newcommand{\DataTypeTok}[1]{\textcolor[rgb]{0.56,0.13,0.00}{{#1}}}
    \newcommand{\DecValTok}[1]{\textcolor[rgb]{0.25,0.63,0.44}{{#1}}}
    \newcommand{\BaseNTok}[1]{\textcolor[rgb]{0.25,0.63,0.44}{{#1}}}
    \newcommand{\FloatTok}[1]{\textcolor[rgb]{0.25,0.63,0.44}{{#1}}}
    \newcommand{\CharTok}[1]{\textcolor[rgb]{0.25,0.44,0.63}{{#1}}}
    \newcommand{\StringTok}[1]{\textcolor[rgb]{0.25,0.44,0.63}{{#1}}}
    \newcommand{\CommentTok}[1]{\textcolor[rgb]{0.38,0.63,0.69}{\textit{{#1}}}}
    \newcommand{\OtherTok}[1]{\textcolor[rgb]{0.00,0.44,0.13}{{#1}}}
    \newcommand{\AlertTok}[1]{\textcolor[rgb]{1.00,0.00,0.00}{\textbf{{#1}}}}
    \newcommand{\FunctionTok}[1]{\textcolor[rgb]{0.02,0.16,0.49}{{#1}}}
    \newcommand{\RegionMarkerTok}[1]{{#1}}
    \newcommand{\ErrorTok}[1]{\textcolor[rgb]{1.00,0.00,0.00}{\textbf{{#1}}}}
    \newcommand{\NormalTok}[1]{{#1}}

    % Additional commands for more recent versions of Pandoc
    \newcommand{\ConstantTok}[1]{\textcolor[rgb]{0.53,0.00,0.00}{{#1}}}
    \newcommand{\SpecialCharTok}[1]{\textcolor[rgb]{0.25,0.44,0.63}{{#1}}}
    \newcommand{\VerbatimStringTok}[1]{\textcolor[rgb]{0.25,0.44,0.63}{{#1}}}
    \newcommand{\SpecialStringTok}[1]{\textcolor[rgb]{0.73,0.40,0.53}{{#1}}}
    \newcommand{\ImportTok}[1]{{#1}}
    \newcommand{\DocumentationTok}[1]{\textcolor[rgb]{0.73,0.13,0.13}{\textit{{#1}}}}
    \newcommand{\AnnotationTok}[1]{\textcolor[rgb]{0.38,0.63,0.69}{\textbf{\textit{{#1}}}}}
    \newcommand{\CommentVarTok}[1]{\textcolor[rgb]{0.38,0.63,0.69}{\textbf{\textit{{#1}}}}}
    \newcommand{\VariableTok}[1]{\textcolor[rgb]{0.10,0.09,0.49}{{#1}}}
    \newcommand{\ControlFlowTok}[1]{\textcolor[rgb]{0.00,0.44,0.13}{\textbf{{#1}}}}
    \newcommand{\OperatorTok}[1]{\textcolor[rgb]{0.40,0.40,0.40}{{#1}}}
    \newcommand{\BuiltInTok}[1]{{#1}}
    \newcommand{\ExtensionTok}[1]{{#1}}
    \newcommand{\PreprocessorTok}[1]{\textcolor[rgb]{0.74,0.48,0.00}{{#1}}}
    \newcommand{\AttributeTok}[1]{\textcolor[rgb]{0.49,0.56,0.16}{{#1}}}
    \newcommand{\InformationTok}[1]{\textcolor[rgb]{0.38,0.63,0.69}{\textbf{\textit{{#1}}}}}
    \newcommand{\WarningTok}[1]{\textcolor[rgb]{0.38,0.63,0.69}{\textbf{\textit{{#1}}}}}


    % Define a nice break command that doesn't care if a line doesn't already
    % exist.
    \def\br{\hspace*{\fill} \\* }
    % Math Jax compatibility definitions
    \def\gt{>}
    \def\lt{<}
    \let\Oldtex\TeX
    \let\Oldlatex\LaTeX
    \renewcommand{\TeX}{\textrm{\Oldtex}}
    \renewcommand{\LaTeX}{\textrm{\Oldlatex}}
    % Document parameters
    % Document title
    \title{Tarea3}
    
    
    
    
    
    
    
% Pygments definitions
\makeatletter
\def\PY@reset{\let\PY@it=\relax \let\PY@bf=\relax%
    \let\PY@ul=\relax \let\PY@tc=\relax%
    \let\PY@bc=\relax \let\PY@ff=\relax}
\def\PY@tok#1{\csname PY@tok@#1\endcsname}
\def\PY@toks#1+{\ifx\relax#1\empty\else%
    \PY@tok{#1}\expandafter\PY@toks\fi}
\def\PY@do#1{\PY@bc{\PY@tc{\PY@ul{%
    \PY@it{\PY@bf{\PY@ff{#1}}}}}}}
\def\PY#1#2{\PY@reset\PY@toks#1+\relax+\PY@do{#2}}

\@namedef{PY@tok@w}{\def\PY@tc##1{\textcolor[rgb]{0.73,0.73,0.73}{##1}}}
\@namedef{PY@tok@c}{\let\PY@it=\textit\def\PY@tc##1{\textcolor[rgb]{0.24,0.48,0.48}{##1}}}
\@namedef{PY@tok@cp}{\def\PY@tc##1{\textcolor[rgb]{0.61,0.40,0.00}{##1}}}
\@namedef{PY@tok@k}{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\@namedef{PY@tok@kp}{\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\@namedef{PY@tok@kt}{\def\PY@tc##1{\textcolor[rgb]{0.69,0.00,0.25}{##1}}}
\@namedef{PY@tok@o}{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\@namedef{PY@tok@ow}{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.67,0.13,1.00}{##1}}}
\@namedef{PY@tok@nb}{\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\@namedef{PY@tok@nf}{\def\PY@tc##1{\textcolor[rgb]{0.00,0.00,1.00}{##1}}}
\@namedef{PY@tok@nc}{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.00,1.00}{##1}}}
\@namedef{PY@tok@nn}{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.00,1.00}{##1}}}
\@namedef{PY@tok@ne}{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.80,0.25,0.22}{##1}}}
\@namedef{PY@tok@nv}{\def\PY@tc##1{\textcolor[rgb]{0.10,0.09,0.49}{##1}}}
\@namedef{PY@tok@no}{\def\PY@tc##1{\textcolor[rgb]{0.53,0.00,0.00}{##1}}}
\@namedef{PY@tok@nl}{\def\PY@tc##1{\textcolor[rgb]{0.46,0.46,0.00}{##1}}}
\@namedef{PY@tok@ni}{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.44,0.44,0.44}{##1}}}
\@namedef{PY@tok@na}{\def\PY@tc##1{\textcolor[rgb]{0.41,0.47,0.13}{##1}}}
\@namedef{PY@tok@nt}{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\@namedef{PY@tok@nd}{\def\PY@tc##1{\textcolor[rgb]{0.67,0.13,1.00}{##1}}}
\@namedef{PY@tok@s}{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\@namedef{PY@tok@sd}{\let\PY@it=\textit\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\@namedef{PY@tok@si}{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.64,0.35,0.47}{##1}}}
\@namedef{PY@tok@se}{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.67,0.36,0.12}{##1}}}
\@namedef{PY@tok@sr}{\def\PY@tc##1{\textcolor[rgb]{0.64,0.35,0.47}{##1}}}
\@namedef{PY@tok@ss}{\def\PY@tc##1{\textcolor[rgb]{0.10,0.09,0.49}{##1}}}
\@namedef{PY@tok@sx}{\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\@namedef{PY@tok@m}{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\@namedef{PY@tok@gh}{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.00,0.50}{##1}}}
\@namedef{PY@tok@gu}{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.50,0.00,0.50}{##1}}}
\@namedef{PY@tok@gd}{\def\PY@tc##1{\textcolor[rgb]{0.63,0.00,0.00}{##1}}}
\@namedef{PY@tok@gi}{\def\PY@tc##1{\textcolor[rgb]{0.00,0.52,0.00}{##1}}}
\@namedef{PY@tok@gr}{\def\PY@tc##1{\textcolor[rgb]{0.89,0.00,0.00}{##1}}}
\@namedef{PY@tok@ge}{\let\PY@it=\textit}
\@namedef{PY@tok@gs}{\let\PY@bf=\textbf}
\@namedef{PY@tok@gp}{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.00,0.50}{##1}}}
\@namedef{PY@tok@go}{\def\PY@tc##1{\textcolor[rgb]{0.44,0.44,0.44}{##1}}}
\@namedef{PY@tok@gt}{\def\PY@tc##1{\textcolor[rgb]{0.00,0.27,0.87}{##1}}}
\@namedef{PY@tok@err}{\def\PY@bc##1{{\setlength{\fboxsep}{\string -\fboxrule}\fcolorbox[rgb]{1.00,0.00,0.00}{1,1,1}{\strut ##1}}}}
\@namedef{PY@tok@kc}{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\@namedef{PY@tok@kd}{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\@namedef{PY@tok@kn}{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\@namedef{PY@tok@kr}{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\@namedef{PY@tok@bp}{\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\@namedef{PY@tok@fm}{\def\PY@tc##1{\textcolor[rgb]{0.00,0.00,1.00}{##1}}}
\@namedef{PY@tok@vc}{\def\PY@tc##1{\textcolor[rgb]{0.10,0.09,0.49}{##1}}}
\@namedef{PY@tok@vg}{\def\PY@tc##1{\textcolor[rgb]{0.10,0.09,0.49}{##1}}}
\@namedef{PY@tok@vi}{\def\PY@tc##1{\textcolor[rgb]{0.10,0.09,0.49}{##1}}}
\@namedef{PY@tok@vm}{\def\PY@tc##1{\textcolor[rgb]{0.10,0.09,0.49}{##1}}}
\@namedef{PY@tok@sa}{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\@namedef{PY@tok@sb}{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\@namedef{PY@tok@sc}{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\@namedef{PY@tok@dl}{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\@namedef{PY@tok@s2}{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\@namedef{PY@tok@sh}{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\@namedef{PY@tok@s1}{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\@namedef{PY@tok@mb}{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\@namedef{PY@tok@mf}{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\@namedef{PY@tok@mh}{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\@namedef{PY@tok@mi}{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\@namedef{PY@tok@il}{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\@namedef{PY@tok@mo}{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\@namedef{PY@tok@ch}{\let\PY@it=\textit\def\PY@tc##1{\textcolor[rgb]{0.24,0.48,0.48}{##1}}}
\@namedef{PY@tok@cm}{\let\PY@it=\textit\def\PY@tc##1{\textcolor[rgb]{0.24,0.48,0.48}{##1}}}
\@namedef{PY@tok@cpf}{\let\PY@it=\textit\def\PY@tc##1{\textcolor[rgb]{0.24,0.48,0.48}{##1}}}
\@namedef{PY@tok@c1}{\let\PY@it=\textit\def\PY@tc##1{\textcolor[rgb]{0.24,0.48,0.48}{##1}}}
\@namedef{PY@tok@cs}{\let\PY@it=\textit\def\PY@tc##1{\textcolor[rgb]{0.24,0.48,0.48}{##1}}}

\def\PYZbs{\char`\\}
\def\PYZus{\char`\_}
\def\PYZob{\char`\{}
\def\PYZcb{\char`\}}
\def\PYZca{\char`\^}
\def\PYZam{\char`\&}
\def\PYZlt{\char`\<}
\def\PYZgt{\char`\>}
\def\PYZsh{\char`\#}
\def\PYZpc{\char`\%}
\def\PYZdl{\char`\$}
\def\PYZhy{\char`\-}
\def\PYZsq{\char`\'}
\def\PYZdq{\char`\"}
\def\PYZti{\char`\~}
% for compatibility with earlier versions
\def\PYZat{@}
\def\PYZlb{[}
\def\PYZrb{]}
\makeatother


    % For linebreaks inside Verbatim environment from package fancyvrb.
    \makeatletter
        \newbox\Wrappedcontinuationbox
        \newbox\Wrappedvisiblespacebox
        \newcommand*\Wrappedvisiblespace {\textcolor{red}{\textvisiblespace}}
        \newcommand*\Wrappedcontinuationsymbol {\textcolor{red}{\llap{\tiny$\m@th\hookrightarrow$}}}
        \newcommand*\Wrappedcontinuationindent {3ex }
        \newcommand*\Wrappedafterbreak {\kern\Wrappedcontinuationindent\copy\Wrappedcontinuationbox}
        % Take advantage of the already applied Pygments mark-up to insert
        % potential linebreaks for TeX processing.
        %        {, <, #, %, $, ' and ": go to next line.
        %        _, }, ^, &, >, - and ~: stay at end of broken line.
        % Use of \textquotesingle for straight quote.
        \newcommand*\Wrappedbreaksatspecials {%
            \def\PYGZus{\discretionary{\char`\_}{\Wrappedafterbreak}{\char`\_}}%
            \def\PYGZob{\discretionary{}{\Wrappedafterbreak\char`\{}{\char`\{}}%
            \def\PYGZcb{\discretionary{\char`\}}{\Wrappedafterbreak}{\char`\}}}%
            \def\PYGZca{\discretionary{\char`\^}{\Wrappedafterbreak}{\char`\^}}%
            \def\PYGZam{\discretionary{\char`\&}{\Wrappedafterbreak}{\char`\&}}%
            \def\PYGZlt{\discretionary{}{\Wrappedafterbreak\char`\<}{\char`\<}}%
            \def\PYGZgt{\discretionary{\char`\>}{\Wrappedafterbreak}{\char`\>}}%
            \def\PYGZsh{\discretionary{}{\Wrappedafterbreak\char`\#}{\char`\#}}%
            \def\PYGZpc{\discretionary{}{\Wrappedafterbreak\char`\%}{\char`\%}}%
            \def\PYGZdl{\discretionary{}{\Wrappedafterbreak\char`\$}{\char`\$}}%
            \def\PYGZhy{\discretionary{\char`\-}{\Wrappedafterbreak}{\char`\-}}%
            \def\PYGZsq{\discretionary{}{\Wrappedafterbreak\textquotesingle}{\textquotesingle}}%
            \def\PYGZdq{\discretionary{}{\Wrappedafterbreak\char`\"}{\char`\"}}%
            \def\PYGZti{\discretionary{\char`\~}{\Wrappedafterbreak}{\char`\~}}%
        }
        % Some characters . , ; ? ! / are not pygmentized.
        % This macro makes them "active" and they will insert potential linebreaks
        \newcommand*\Wrappedbreaksatpunct {%
            \lccode`\~`\.\lowercase{\def~}{\discretionary{\hbox{\char`\.}}{\Wrappedafterbreak}{\hbox{\char`\.}}}%
            \lccode`\~`\,\lowercase{\def~}{\discretionary{\hbox{\char`\,}}{\Wrappedafterbreak}{\hbox{\char`\,}}}%
            \lccode`\~`\;\lowercase{\def~}{\discretionary{\hbox{\char`\;}}{\Wrappedafterbreak}{\hbox{\char`\;}}}%
            \lccode`\~`\:\lowercase{\def~}{\discretionary{\hbox{\char`\:}}{\Wrappedafterbreak}{\hbox{\char`\:}}}%
            \lccode`\~`\?\lowercase{\def~}{\discretionary{\hbox{\char`\?}}{\Wrappedafterbreak}{\hbox{\char`\?}}}%
            \lccode`\~`\!\lowercase{\def~}{\discretionary{\hbox{\char`\!}}{\Wrappedafterbreak}{\hbox{\char`\!}}}%
            \lccode`\~`\/\lowercase{\def~}{\discretionary{\hbox{\char`\/}}{\Wrappedafterbreak}{\hbox{\char`\/}}}%
            \catcode`\.\active
            \catcode`\,\active
            \catcode`\;\active
            \catcode`\:\active
            \catcode`\?\active
            \catcode`\!\active
            \catcode`\/\active
            \lccode`\~`\~
        }
    \makeatother

    \let\OriginalVerbatim=\Verbatim
    \makeatletter
    \renewcommand{\Verbatim}[1][1]{%
        %\parskip\z@skip
        \sbox\Wrappedcontinuationbox {\Wrappedcontinuationsymbol}%
        \sbox\Wrappedvisiblespacebox {\FV@SetupFont\Wrappedvisiblespace}%
        \def\FancyVerbFormatLine ##1{\hsize\linewidth
            \vtop{\raggedright\hyphenpenalty\z@\exhyphenpenalty\z@
                \doublehyphendemerits\z@\finalhyphendemerits\z@
                \strut ##1\strut}%
        }%
        % If the linebreak is at a space, the latter will be displayed as visible
        % space at end of first line, and a continuation symbol starts next line.
        % Stretch/shrink are however usually zero for typewriter font.
        \def\FV@Space {%
            \nobreak\hskip\z@ plus\fontdimen3\font minus\fontdimen4\font
            \discretionary{\copy\Wrappedvisiblespacebox}{\Wrappedafterbreak}
            {\kern\fontdimen2\font}%
        }%

        % Allow breaks at special characters using \PYG... macros.
        \Wrappedbreaksatspecials
        % Breaks at punctuation characters . , ; ? ! and / need catcode=\active
        \OriginalVerbatim[#1,codes*=\Wrappedbreaksatpunct]%
    }
    \makeatother

    % Exact colors from NB
    \definecolor{incolor}{HTML}{303F9F}
    \definecolor{outcolor}{HTML}{D84315}
    \definecolor{cellborder}{HTML}{CFCFCF}
    \definecolor{cellbackground}{HTML}{F7F7F7}

    % prompt
    \makeatletter
    \newcommand{\boxspacing}{\kern\kvtcb@left@rule\kern\kvtcb@boxsep}
    \makeatother
    \newcommand{\prompt}[4]{
        {\ttfamily\llap{{\color{#2}[#3]:\hspace{3pt}#4}}\vspace{-\baselineskip}}
    }
    

    
    % Prevent overflowing lines due to hard-to-break entities
    \sloppy
    % Setup hyperref package
    \hypersetup{
      breaklinks=true,  % so long urls are correctly broken across lines
      colorlinks=true,
      urlcolor=urlcolor,
      linkcolor=linkcolor,
      citecolor=citecolor,
      }
    % Slightly bigger margins than the latex defaults
    
    \geometry{verbose,tmargin=1in,bmargin=1in,lmargin=1in,rmargin=1in}
    
    

\begin{document}
    
    \maketitle
    
    

    
    \section{Tarea 3}\label{tarea-3}

Jairo Saul Diaz Soto

Dr.~Joaquin Peña Acevedo

Optimizacion I

2024 / 02 / 18

    \begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\subsection{Ejercicio 1}\label{ejercicio-1}

    \subsubsection{1.- Backtracking y condicion de
Armijo}\label{backtracking-y-condicion-de-armijo}

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{39}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{k+kn}{import} \PY{n+nn}{numpy} \PY{k}{as} \PY{n+nn}{np}
\PY{k+kn}{import} \PY{n+nn}{matplotlib}\PY{n+nn}{.}\PY{n+nn}{pyplot} \PY{k}{as} \PY{n+nn}{plt}
\end{Verbatim}
\end{tcolorbox}

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{40}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{k}{def} \PY{n+nf}{bt\PYZus{}armijo}\PY{p}{(}\PY{n}{a0}\PY{p}{,} \PY{n}{rho}\PY{p}{,} \PY{n}{c0}\PY{p}{,} \PY{n}{x0}\PY{p}{,} \PY{n}{f\PYZus{}func}\PY{p}{,} \PY{n}{grad\PYZus{}fun}\PY{p}{,} \PY{n}{p0}\PY{p}{,} \PY{n}{NMax}\PY{p}{)}\PY{p}{:}
    \PY{n}{a} \PY{o}{=} \PY{n}{a0}
    \PY{k}{for} \PY{n}{k} \PY{o+ow}{in} \PY{n+nb}{range}\PY{p}{(}\PY{n}{NMax}\PY{p}{)}\PY{p}{:}
        \PY{k}{if} \PY{n}{f\PYZus{}func}\PY{p}{(}\PY{n}{x0} \PY{o}{+} \PY{p}{(}\PY{n}{a}\PY{o}{*}\PY{n}{p0}\PY{p}{)}\PY{p}{)} \PY{o}{\PYZlt{}}\PY{o}{=} \PY{n}{f\PYZus{}func}\PY{p}{(}\PY{n}{x0}\PY{p}{)} \PY{o}{+} \PY{p}{(}\PY{n}{c0}\PY{o}{*}\PY{n}{a}\PY{o}{*}\PY{p}{(}\PY{n}{grad\PYZus{}fun}\PY{p}{(}\PY{n}{x0}\PY{p}{)}\PY{n+nd}{@p0}\PY{p}{)}\PY{p}{)}\PY{p}{:}
            \PY{k}{return} \PY{n}{a}\PY{p}{,} \PY{n}{k}
        \PY{n}{a} \PY{o}{*}\PY{o}{=} \PY{n}{rho}

    
\end{Verbatim}
\end{tcolorbox}

    \subsubsection{2.- Descenso con
backtracking}\label{descenso-con-backtracking}

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{41}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{k}{def} \PY{n+nf}{descmax\PYZus{}bt}\PY{p}{(}\PY{n}{f\PYZus{}fun}\PY{p}{,} \PY{n}{grad\PYZus{}fun}\PY{p}{,} \PY{n}{x0}\PY{p}{,} \PY{n}{tau}\PY{p}{,} \PY{n}{NMax}\PY{p}{,} \PY{n}{a0}\PY{p}{,} \PY{n}{rho}\PY{p}{,} \PY{n}{c0}\PY{p}{,} \PY{n}{KMax}\PY{p}{)}\PY{p}{:}
    \PY{n}{record} \PY{o}{=} \PY{n+nb}{list}\PY{p}{(}\PY{p}{)}
    \PY{k}{for} \PY{n}{k} \PY{o+ow}{in} \PY{n+nb}{range}\PY{p}{(}\PY{n}{NMax}\PY{p}{)}\PY{p}{:}
        \PY{n}{g} \PY{o}{=} \PY{n}{grad\PYZus{}fun}\PY{p}{(}\PY{n}{x0}\PY{p}{)}
        \PY{n}{p} \PY{o}{=} \PY{o}{\PYZhy{}}\PY{n}{g}
        \PY{n}{a}\PY{p}{,} \PY{n}{i} \PY{o}{=} \PY{n}{bt\PYZus{}armijo}\PY{p}{(}\PY{n}{a0}\PY{p}{,} \PY{n}{rho}\PY{p}{,} \PY{n}{c0}\PY{p}{,} \PY{n}{x0}\PY{p}{,} \PY{n}{f\PYZus{}fun}\PY{p}{,} \PY{n}{grad\PYZus{}fun}\PY{p}{,} \PY{n}{p}\PY{p}{,} \PY{n}{KMax}\PY{p}{)}
        \PY{k}{if} \PY{n+nb}{len}\PY{p}{(}\PY{n}{x0}\PY{p}{)} \PY{o}{==} \PY{l+m+mi}{2}\PY{p}{:}
            \PY{n}{record}\PY{o}{.}\PY{n}{append}\PY{p}{(}\PY{p}{[}\PY{n}{x0}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{]}\PY{p}{,} \PY{n}{x0}\PY{p}{[}\PY{l+m+mi}{1}\PY{p}{]}\PY{p}{,} \PY{n}{a}\PY{p}{,} \PY{n}{i}\PY{p}{]}\PY{p}{)}
        \PY{k}{if} \PY{n}{a} \PY{o}{*} \PY{n}{np}\PY{o}{.}\PY{n}{linalg}\PY{o}{.}\PY{n}{norm}\PY{p}{(}\PY{n}{p}\PY{p}{)} \PY{o}{\PYZlt{}} \PY{n}{tau}\PY{p}{:}
            \PY{k}{return} \PY{n}{x0}\PY{p}{,} \PY{n}{k}\PY{p}{,} \PY{k+kc}{True}\PY{p}{,} \PY{n}{np}\PY{o}{.}\PY{n}{array}\PY{p}{(}\PY{n}{record}\PY{p}{)}
        \PY{n}{x0} \PY{o}{+}\PY{o}{=} \PY{n}{a}\PY{o}{*}\PY{n}{p}
    \PY{k}{return} \PY{n}{x0}\PY{p}{,} \PY{n}{k}\PY{p}{,} \PY{k+kc}{False}\PY{p}{,} \PY{n}{np}\PY{o}{.}\PY{n}{array}\PY{p}{(}\PY{n}{record}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

    \subsubsection{3.- Probando el algoritmo}\label{probando-el-algoritmo}

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{42}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{k}{def} \PY{n+nf}{himmelblau}\PY{p}{(}\PY{n}{x}\PY{p}{)}\PY{p}{:}
    \PY{k}{return} \PY{p}{(}\PY{p}{(}\PY{n}{x}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{]}\PY{o}{*}\PY{o}{*}\PY{l+m+mi}{2}\PY{p}{)} \PY{o}{+} \PY{n}{x}\PY{p}{[}\PY{l+m+mi}{1}\PY{p}{]} \PY{o}{\PYZhy{}} \PY{l+m+mi}{11}\PY{p}{)}\PY{o}{*}\PY{o}{*}\PY{l+m+mi}{2} \PY{o}{+} \PY{p}{(}\PY{n}{x}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{]} \PY{o}{+} \PY{p}{(}\PY{n}{x}\PY{p}{[}\PY{l+m+mi}{1}\PY{p}{]}\PY{o}{*}\PY{o}{*}\PY{l+m+mi}{2}\PY{p}{)} \PY{o}{\PYZhy{}} \PY{l+m+mi}{7}\PY{p}{)}\PY{o}{*}\PY{o}{*}\PY{l+m+mi}{2}

\PY{k}{def} \PY{n+nf}{himmelblau\PYZus{}grad}\PY{p}{(}\PY{n}{x}\PY{p}{)}\PY{p}{:}
    \PY{k}{return} \PY{n}{np}\PY{o}{.}\PY{n}{array}\PY{p}{(}\PY{p}{[}\PY{p}{(}\PY{l+m+mi}{4}\PY{o}{*}\PY{n}{x}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{]}\PY{o}{*}\PY{p}{(}\PY{p}{(}\PY{n}{x}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{]}\PY{o}{*}\PY{o}{*}\PY{l+m+mi}{2}\PY{p}{)} \PY{o}{+} \PY{n}{x}\PY{p}{[}\PY{l+m+mi}{1}\PY{p}{]} \PY{o}{\PYZhy{}} \PY{l+m+mi}{11}\PY{p}{)}\PY{p}{)} \PY{o}{+} \PY{p}{(}\PY{l+m+mi}{2}\PY{o}{*}\PY{p}{(}\PY{n}{x}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{]} \PY{o}{+} \PY{p}{(}\PY{n}{x}\PY{p}{[}\PY{l+m+mi}{1}\PY{p}{]}\PY{o}{*}\PY{o}{*}\PY{l+m+mi}{2}\PY{p}{)} \PY{o}{\PYZhy{}} \PY{l+m+mi}{7}\PY{p}{)}\PY{p}{)}\PY{p}{,}
                     \PY{p}{(}\PY{l+m+mi}{2}\PY{o}{*}\PY{p}{(}\PY{p}{(}\PY{n}{x}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{]}\PY{o}{*}\PY{o}{*}\PY{l+m+mi}{2}\PY{p}{)} \PY{o}{+} \PY{n}{x}\PY{p}{[}\PY{l+m+mi}{1}\PY{p}{]} \PY{o}{\PYZhy{}} \PY{l+m+mi}{11}\PY{p}{)}\PY{p}{)} \PY{o}{+} \PY{p}{(}\PY{l+m+mi}{4}\PY{o}{*}\PY{n}{x}\PY{p}{[}\PY{l+m+mi}{1}\PY{p}{]}\PY{o}{*}\PY{p}{(}\PY{n}{x}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{]} \PY{o}{+} \PY{p}{(}\PY{n}{x}\PY{p}{[}\PY{l+m+mi}{1}\PY{p}{]}\PY{o}{*}\PY{o}{*}\PY{l+m+mi}{2}\PY{p}{)} \PY{o}{\PYZhy{}} \PY{l+m+mi}{7}\PY{p}{)}\PY{p}{)}\PY{p}{]}\PY{p}{)}

\PY{k}{def} \PY{n+nf}{bale}\PY{p}{(}\PY{n}{x}\PY{p}{)}\PY{p}{:}
    \PY{k}{return} \PY{p}{(}\PY{l+m+mf}{1.5} \PY{o}{\PYZhy{}} \PY{n}{x}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{]} \PY{o}{+} \PY{p}{(}\PY{n}{x}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{]}\PY{o}{*}\PY{n}{x}\PY{p}{[}\PY{l+m+mi}{1}\PY{p}{]}\PY{p}{)}\PY{p}{)}\PY{o}{*}\PY{o}{*}\PY{l+m+mi}{2} \PY{o}{+} \PY{p}{(}\PY{l+m+mf}{2.25} \PY{o}{\PYZhy{}} \PY{n}{x}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{]} \PY{o}{+} \PY{p}{(}\PY{n}{x}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{]}\PY{o}{*}\PY{n}{x}\PY{p}{[}\PY{l+m+mi}{1}\PY{p}{]}\PY{o}{*}\PY{o}{*}\PY{l+m+mi}{2}\PY{p}{)}\PY{p}{)}\PY{o}{*}\PY{o}{*}\PY{l+m+mi}{2} \PY{o}{+} \PY{p}{(}\PY{l+m+mf}{2.625} \PY{o}{\PYZhy{}} \PY{n}{x}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{]} \PY{o}{+} \PY{p}{(}\PY{n}{x}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{]}\PY{o}{*}\PY{n}{x}\PY{p}{[}\PY{l+m+mi}{1}\PY{p}{]}\PY{o}{*}\PY{o}{*}\PY{l+m+mi}{3}\PY{p}{)}\PY{p}{)}\PY{o}{*}\PY{o}{*}\PY{l+m+mi}{2}
    
\PY{k}{def} \PY{n+nf}{bale\PYZus{}grad}\PY{p}{(}\PY{n}{x}\PY{p}{)}\PY{p}{:}
    \PY{k}{return} \PY{n}{np}\PY{o}{.}\PY{n}{array}\PY{p}{(}\PY{p}{[} \PY{p}{(}\PY{l+m+mi}{2}\PY{o}{*}\PY{p}{(}\PY{n}{x}\PY{p}{[}\PY{l+m+mi}{1}\PY{p}{]} \PY{o}{\PYZhy{}} \PY{l+m+mi}{1}\PY{p}{)} \PY{o}{*} \PY{p}{(}\PY{l+m+mf}{1.5} \PY{o}{\PYZhy{}} \PY{n}{x}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{]} \PY{o}{+} \PY{p}{(}\PY{n}{x}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{]}\PY{o}{*}\PY{n}{x}\PY{p}{[}\PY{l+m+mi}{1}\PY{p}{]}\PY{p}{)}\PY{p}{)}\PY{p}{)} \PY{o}{+} \PY{p}{(}\PY{l+m+mi}{2}\PY{o}{*}\PY{p}{(}\PY{n}{x}\PY{p}{[}\PY{l+m+mi}{1}\PY{p}{]}\PY{o}{*}\PY{o}{*}\PY{l+m+mi}{2} \PY{o}{\PYZhy{}} \PY{l+m+mi}{1}\PY{p}{)}\PY{o}{*}\PY{p}{(}\PY{l+m+mf}{2.25} \PY{o}{\PYZhy{}} \PY{n}{x}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{]} \PY{o}{+} \PY{p}{(}\PY{n}{x}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{]}\PY{o}{*}\PY{n}{x}\PY{p}{[}\PY{l+m+mi}{1}\PY{p}{]}\PY{o}{*}\PY{o}{*}\PY{l+m+mi}{2}\PY{p}{)}\PY{p}{)}\PY{p}{)} \PY{o}{+} \PY{p}{(}\PY{l+m+mi}{2}\PY{o}{*}\PY{p}{(}\PY{n}{x}\PY{p}{[}\PY{l+m+mi}{1}\PY{p}{]}\PY{o}{*}\PY{o}{*}\PY{l+m+mi}{3} \PY{o}{\PYZhy{}} \PY{l+m+mi}{1}\PY{p}{)}\PY{o}{*}\PY{p}{(}\PY{l+m+mf}{2.625} \PY{o}{\PYZhy{}} \PY{n}{x}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{]} \PY{o}{+} \PY{p}{(}\PY{n}{x}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{]}\PY{o}{*}\PY{n}{x}\PY{p}{[}\PY{l+m+mi}{1}\PY{p}{]}\PY{o}{*}\PY{o}{*}\PY{l+m+mi}{3}\PY{p}{)}\PY{p}{)}\PY{p}{)}\PY{p}{,} 
                    \PY{p}{(}\PY{l+m+mi}{2}\PY{o}{*}\PY{n}{x}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{]}\PY{o}{*}\PY{p}{(}\PY{l+m+mf}{1.5} \PY{o}{\PYZhy{}} \PY{n}{x}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{]} \PY{o}{+} \PY{p}{(}\PY{n}{x}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{]}\PY{o}{*}\PY{n}{x}\PY{p}{[}\PY{l+m+mi}{1}\PY{p}{]}\PY{p}{)}\PY{p}{)}\PY{p}{)} \PY{o}{+} \PY{p}{(}\PY{l+m+mi}{4}\PY{o}{*}\PY{p}{(}\PY{n}{x}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{]}\PY{o}{*}\PY{n}{x}\PY{p}{[}\PY{l+m+mi}{1}\PY{p}{]}\PY{p}{)}\PY{o}{*}\PY{p}{(}\PY{l+m+mf}{2.25} \PY{o}{\PYZhy{}} \PY{n}{x}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{]} \PY{o}{+} \PY{p}{(}\PY{n}{x}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{]}\PY{o}{*}\PY{n}{x}\PY{p}{[}\PY{l+m+mi}{1}\PY{p}{]}\PY{o}{*}\PY{o}{*}\PY{l+m+mi}{2}\PY{p}{)}\PY{p}{)}\PY{p}{)} \PY{o}{+} \PY{p}{(}\PY{l+m+mi}{6}\PY{o}{*}\PY{p}{(}\PY{n}{x}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{]}\PY{o}{*}\PY{n}{x}\PY{p}{[}\PY{l+m+mi}{1}\PY{p}{]}\PY{o}{*}\PY{o}{*}\PY{l+m+mi}{2}\PY{p}{)}\PY{o}{*}\PY{p}{(}\PY{l+m+mf}{2.625} \PY{o}{\PYZhy{}} \PY{n}{x}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{]} \PY{o}{+} \PY{p}{(}\PY{n}{x}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{]}\PY{o}{*}\PY{n}{x}\PY{p}{[}\PY{l+m+mi}{1}\PY{p}{]}\PY{o}{*}\PY{o}{*}\PY{l+m+mi}{3}\PY{p}{)}\PY{p}{)}\PY{p}{)}\PY{p}{]}\PY{p}{)}

\PY{k}{def} \PY{n+nf}{rosenbrock}\PY{p}{(}\PY{n}{x}\PY{p}{)}\PY{p}{:}
    \PY{n}{n} \PY{o}{=} \PY{n+nb}{len}\PY{p}{(}\PY{n}{x}\PY{p}{)}
    \PY{n}{res} \PY{o}{=} \PY{l+m+mi}{0}
    \PY{k}{for} \PY{n}{k} \PY{o+ow}{in} \PY{n+nb}{range}\PY{p}{(}\PY{n}{n}\PY{o}{\PYZhy{}}\PY{l+m+mi}{1}\PY{p}{)}\PY{p}{:}
        \PY{n}{res} \PY{o}{+}\PY{o}{=} \PY{p}{(}\PY{l+m+mi}{100} \PY{o}{*} \PY{p}{(}\PY{n}{x}\PY{p}{[}\PY{n}{k}\PY{o}{+}\PY{l+m+mi}{1}\PY{p}{]} \PY{o}{\PYZhy{}} \PY{p}{(}\PY{n}{x}\PY{p}{[}\PY{n}{k}\PY{p}{]}\PY{o}{*}\PY{o}{*}\PY{l+m+mi}{2}\PY{p}{)}\PY{p}{)}\PY{o}{*}\PY{o}{*}\PY{l+m+mi}{2} \PY{p}{)} \PY{o}{+} \PY{p}{(}\PY{l+m+mi}{1}\PY{o}{\PYZhy{}}\PY{n}{x}\PY{p}{[}\PY{n}{k}\PY{p}{]}\PY{p}{)}\PY{o}{*}\PY{o}{*}\PY{l+m+mi}{2}
    \PY{k}{return} \PY{n}{res}

\PY{k}{def} \PY{n+nf}{rosenbrock\PYZus{}grad}\PY{p}{(}\PY{n}{x}\PY{p}{)}\PY{p}{:}
    \PY{n}{gradient} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{zeros\PYZus{}like}\PY{p}{(}\PY{n}{x}\PY{p}{)}
    \PY{n}{n} \PY{o}{=} \PY{n+nb}{len}\PY{p}{(}\PY{n}{x}\PY{p}{)}
    \PY{k}{for} \PY{n}{i} \PY{o+ow}{in} \PY{n+nb}{range}\PY{p}{(}\PY{n}{n}\PY{o}{\PYZhy{}}\PY{l+m+mi}{1}\PY{p}{)}\PY{p}{:}
        \PY{n}{gradient}\PY{p}{[}\PY{n}{i}\PY{p}{]} \PY{o}{+}\PY{o}{=} \PY{p}{(}\PY{o}{\PYZhy{}}\PY{l+m+mi}{400}\PY{o}{*}\PY{n}{x}\PY{p}{[}\PY{n}{i}\PY{p}{]}\PY{o}{*}\PY{p}{(}\PY{n}{x}\PY{p}{[}\PY{n}{i}\PY{o}{+}\PY{l+m+mi}{1}\PY{p}{]} \PY{o}{\PYZhy{}} \PY{p}{(}\PY{n}{x}\PY{p}{[}\PY{n}{i}\PY{p}{]}\PY{o}{*}\PY{o}{*}\PY{l+m+mi}{2}\PY{p}{)}\PY{p}{)}\PY{p}{)} \PY{o}{\PYZhy{}} \PY{p}{(}\PY{l+m+mi}{2}\PY{o}{*}\PY{p}{(}\PY{l+m+mi}{1}\PY{o}{\PYZhy{}}\PY{n}{x}\PY{p}{[}\PY{n}{i}\PY{p}{]}\PY{p}{)}\PY{p}{)}
        \PY{n}{gradient}\PY{p}{[}\PY{n}{i}\PY{o}{+}\PY{l+m+mi}{1}\PY{p}{]} \PY{o}{+}\PY{o}{=} \PY{p}{(}\PY{l+m+mi}{200}\PY{p}{)}\PY{o}{*}\PY{p}{(}\PY{n}{x}\PY{p}{[}\PY{n}{i}\PY{o}{+}\PY{l+m+mi}{1}\PY{p}{]} \PY{o}{\PYZhy{}} \PY{p}{(}\PY{n}{x}\PY{p}{[}\PY{n}{i}\PY{p}{]}\PY{o}{*}\PY{o}{*}\PY{l+m+mi}{2}\PY{p}{)}\PY{p}{)}
    \PY{k}{return} \PY{n}{gradient}
\end{Verbatim}
\end{tcolorbox}

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{43}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{k}{def} \PY{n+nf}{contornosFnc2D}\PY{p}{(}\PY{n}{fncf}\PY{p}{,} \PY{n}{xleft}\PY{p}{,} \PY{n}{xright}\PY{p}{,} \PY{n}{ybottom}\PY{p}{,} \PY{n}{ytop}\PY{p}{,} \PY{n}{levels}\PY{p}{,} \PY{n}{puntos}\PY{o}{=}\PY{k+kc}{None}\PY{p}{)}\PY{p}{:}
    \PY{c+c1}{\PYZsh{} Crea una discretización uniforme del intervalo [xleft, xright]}
    \PY{n}{ax} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{linspace}\PY{p}{(}\PY{n}{xleft}\PY{p}{,} \PY{n}{xright}\PY{p}{,} \PY{l+m+mi}{250}\PY{p}{)}
    \PY{c+c1}{\PYZsh{} Crea una discretización uniforme del intervalo [ybottom, ytop]}
    \PY{n}{ay} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{linspace}\PY{p}{(}\PY{n}{ybottom}\PY{p}{,} \PY{n}{ytop}\PY{p}{,} \PY{l+m+mi}{200}\PY{p}{)}
    \PY{c+c1}{\PYZsh{} La matriz mX que tiene las abscisas}
    \PY{n}{mX}\PY{p}{,} \PY{n}{mY} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{meshgrid}\PY{p}{(}\PY{n}{ax}\PY{p}{,} \PY{n}{ay}\PY{p}{)}
    \PY{c+c1}{\PYZsh{} Se crea el arreglo mZ con los valores de la función en cada nodo}
    \PY{n}{mZ} \PY{o}{=} \PY{n}{mX}\PY{o}{.}\PY{n}{copy}\PY{p}{(}\PY{p}{)}
    \PY{k}{for} \PY{n}{i}\PY{p}{,} \PY{n}{y} \PY{o+ow}{in} \PY{n+nb}{enumerate}\PY{p}{(}\PY{n}{ay}\PY{p}{)}\PY{p}{:}
        \PY{k}{for} \PY{n}{j}\PY{p}{,} \PY{n}{x} \PY{o+ow}{in} \PY{n+nb}{enumerate}\PY{p}{(}\PY{n}{ax}\PY{p}{)}\PY{p}{:}
            \PY{n}{mZ}\PY{p}{[}\PY{n}{i}\PY{p}{,} \PY{n}{j}\PY{p}{]} \PY{o}{=} \PY{n}{fncf}\PY{p}{(}\PY{n}{np}\PY{o}{.}\PY{n}{array}\PY{p}{(}\PY{p}{[}\PY{n}{x}\PY{p}{,} \PY{n}{y}\PY{p}{]}\PY{p}{)}\PY{p}{)}
    
    \PY{c+c1}{\PYZsh{} Grafica de las curvas de nivel}
    \PY{n}{fig}\PY{p}{,} \PY{n}{ax} \PY{o}{=} \PY{n}{plt}\PY{o}{.}\PY{n}{subplots}\PY{p}{(}\PY{p}{)}
    \PY{n}{CS} \PY{o}{=} \PY{n}{ax}\PY{o}{.}\PY{n}{contour}\PY{p}{(}\PY{n}{mX}\PY{p}{,} \PY{n}{mY}\PY{p}{,} \PY{n}{mZ}\PY{p}{,} \PY{n}{levels}\PY{p}{,} \PY{n}{cmap}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Wistia}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
    
    \PY{c+c1}{\PYZsh{} Grafica los puntos y conecta la secuencia con líneas}
    \PY{k}{if} \PY{n}{puntos} \PY{o+ow}{is} \PY{o+ow}{not} \PY{k+kc}{None}\PY{p}{:}
        \PY{n}{puntos} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{array}\PY{p}{(}\PY{n}{puntos}\PY{p}{)}
        \PY{n}{ax}\PY{o}{.}\PY{n}{plot}\PY{p}{(}\PY{n}{puntos}\PY{p}{[}\PY{p}{:}\PY{p}{,} \PY{l+m+mi}{0}\PY{p}{]}\PY{p}{,} \PY{n}{puntos}\PY{p}{[}\PY{p}{:}\PY{p}{,} \PY{l+m+mi}{1}\PY{p}{]}\PY{p}{,} \PY{n}{color}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{red}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{marker}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{o}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{linestyle}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{\PYZhy{}}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{linewidth}\PY{o}{=}\PY{l+m+mi}{2}\PY{p}{,} \PY{n}{label}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Secuencia de puntos}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
    
    \PY{n}{ax}\PY{o}{.}\PY{n}{legend}\PY{p}{(}\PY{p}{)}  \PY{c+c1}{\PYZsh{} Muestra la leyenda si se han graficado puntos}
    \PY{n}{plt}\PY{o}{.}\PY{n}{show}\PY{p}{(}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{44}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{n}{eps\PYZus{}m} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{finfo}\PY{p}{(}\PY{n+nb}{float}\PY{p}{)}\PY{o}{.}\PY{n}{eps}
\end{Verbatim}
\end{tcolorbox}

    \begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

Funcion de Himmelblau

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{45}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{n}{ffun} \PY{o}{=} \PY{n}{himmelblau}
\PY{n}{gfun} \PY{o}{=} \PY{n}{himmelblau\PYZus{}grad}
\PY{n}{xsearch} \PY{o}{=} \PY{p}{[}\PY{o}{\PYZhy{}}\PY{l+m+mi}{5}\PY{p}{,}\PY{l+m+mi}{5}\PY{p}{]}
\PY{n}{ysearch} \PY{o}{=} \PY{p}{[}\PY{o}{\PYZhy{}}\PY{l+m+mi}{5}\PY{p}{,}\PY{l+m+mi}{5}\PY{p}{]}
\PY{n}{x0} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{array}\PY{p}{(}\PY{p}{[}\PY{l+m+mf}{2.0}\PY{p}{,} \PY{l+m+mf}{4.0}\PY{p}{]}\PY{p}{)}
\PY{n}{n} \PY{o}{=} \PY{n+nb}{len}\PY{p}{(}\PY{n}{x0}\PY{p}{)}
\PY{n}{tau} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{sqrt}\PY{p}{(}\PY{n}{n}\PY{o}{*}\PY{n}{eps\PYZus{}m}\PY{p}{)}
\PY{n}{a0} \PY{o}{=} \PY{l+m+mf}{1.0}
\PY{n}{rho} \PY{o}{=} \PY{l+m+mf}{0.8}
\PY{n}{c0} \PY{o}{=} \PY{l+m+mf}{0.1}
\PY{n}{NMax} \PY{o}{=} \PY{l+m+mi}{30000}
\PY{n}{KMax} \PY{o}{=} \PY{l+m+mi}{600}
\end{Verbatim}
\end{tcolorbox}

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{46}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{n}{xk}\PY{p}{,} \PY{n}{kiter}\PY{p}{,} \PY{n}{bl}\PY{p}{,} \PY{n}{rcrd} \PY{o}{=} \PY{n}{descmax\PYZus{}bt}\PY{p}{(}\PY{n}{ffun}\PY{p}{,} \PY{n}{gfun}\PY{p}{,} \PY{n}{x0}\PY{p}{,} \PY{n}{tau}\PY{p}{,} \PY{n}{NMax}\PY{p}{,} \PY{n}{a0}\PY{p}{,} \PY{n}{rho}\PY{p}{,} \PY{n}{c0}\PY{p}{,} \PY{n}{KMax}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{47}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{n+nb}{print}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{El numero de iteraciones del algoritmo de descenso fue de: }\PY{l+s+s2}{\PYZdq{}}\PY{p}{,}\PY{n}{kiter}\PY{p}{)}
\PY{n+nb}{print}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{El punto optimo hallado fue: }\PY{l+s+s2}{\PYZdq{}}\PY{p}{,}\PY{n}{xk}\PY{p}{)}
\PY{n+nb}{print}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{El valor de la funcion en el punto optimo hallado es: }\PY{l+s+s2}{\PYZdq{}}\PY{p}{,} \PY{n}{ffun}\PY{p}{(}\PY{n}{xk}\PY{p}{)}\PY{p}{)}
\PY{n+nb}{print}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{La norma del gradiente en el punto optimo hallado es de}\PY{l+s+s2}{\PYZdq{}}\PY{p}{,} \PY{n}{np}\PY{o}{.}\PY{n}{linalg}\PY{o}{.}\PY{n}{norm}\PY{p}{(}\PY{n}{gfun}\PY{p}{(}\PY{n}{xk}\PY{p}{)}\PY{p}{)}\PY{p}{)}
\PY{n+nb}{print}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{El algoritmo termino con condicion de paro: }\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{bl}\PY{p}{)}
\PY{n+nb}{print}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{El valor promedio de los pasos fue: }\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{np}\PY{o}{.}\PY{n}{mean}\PY{p}{(}\PY{n}{rcrd}\PY{p}{[}\PY{p}{:}\PY{p}{,} \PY{l+m+mi}{2}\PY{p}{]}\PY{p}{)}\PY{p}{)}
\PY{n+nb}{print}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{El promedio de iteraciones del algoritmo de backtracking fue de: }\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{np}\PY{o}{.}\PY{n}{mean}\PY{p}{(}\PY{n}{rcrd}\PY{p}{[}\PY{p}{:}\PY{p}{,}\PY{o}{\PYZhy{}}\PY{l+m+mi}{1}\PY{p}{:}\PY{p}{]}\PY{p}{)}\PY{p}{)}

\PY{n}{contornosFnc2D}\PY{p}{(}\PY{n}{ffun}\PY{p}{,} \PY{n}{xsearch}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{]}\PY{p}{,} \PY{n}{xsearch}\PY{p}{[}\PY{l+m+mi}{1}\PY{p}{]}\PY{p}{,} \PY{n}{ysearch}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{]}\PY{p}{,} \PY{n}{ysearch}\PY{p}{[}\PY{l+m+mi}{1}\PY{p}{]}\PY{p}{,} \PY{p}{[}\PY{l+m+mf}{0.5}\PY{p}{,} \PY{l+m+mi}{5}\PY{p}{,} \PY{l+m+mi}{10}\PY{p}{,} \PY{l+m+mi}{25}\PY{p}{,} \PY{l+m+mi}{50}\PY{p}{,} \PY{l+m+mi}{100}\PY{p}{,} \PY{l+m+mi}{150}\PY{p}{,} \PY{l+m+mi}{250}\PY{p}{,} \PY{l+m+mi}{400}\PY{p}{]}\PY{p}{,} \PY{n}{rcrd}\PY{p}{[}\PY{p}{:}\PY{p}{,}\PY{p}{:}\PY{l+m+mi}{2}\PY{p}{]}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

    \begin{Verbatim}[commandchars=\\\{\}]
El numero de iteraciones del algoritmo de descenso fue de:  29
El punto optimo hallado fue:  [2.99999999 1.99999999]
El valor de la funcion en el punto optimo hallado es:  7.84550082956209e-15
La norma del gradiente en el punto optimo hallado es de 1.1100948506376434e-06
El algoritmo termino con condicion de paro:  True
El valor promedio de los pasos fue:  0.018802528444271847
El promedio de iteraciones del algoritmo de backtracking fue de:
17.833333333333332
    \end{Verbatim}

    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{Tarea3_files/Tarea3_14_1.png}
    \end{center}
    { \hspace*{\fill} \\}
    
    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{48}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{n}{ffun} \PY{o}{=} \PY{n}{himmelblau}
\PY{n}{gfun} \PY{o}{=} \PY{n}{himmelblau\PYZus{}grad}
\PY{n}{xsearch} \PY{o}{=} \PY{p}{[}\PY{o}{\PYZhy{}}\PY{l+m+mi}{5}\PY{p}{,}\PY{l+m+mi}{5}\PY{p}{]}
\PY{n}{ysearch} \PY{o}{=} \PY{p}{[}\PY{o}{\PYZhy{}}\PY{l+m+mi}{5}\PY{p}{,}\PY{l+m+mi}{5}\PY{p}{]}
\PY{n}{x0} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{array}\PY{p}{(}\PY{p}{[}\PY{l+m+mf}{0.0}\PY{p}{,} \PY{l+m+mf}{0.0}\PY{p}{]}\PY{p}{)}
\PY{n}{n} \PY{o}{=} \PY{n+nb}{len}\PY{p}{(}\PY{n}{x0}\PY{p}{)}
\PY{n}{tau} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{sqrt}\PY{p}{(}\PY{n}{n}\PY{o}{*}\PY{n}{eps\PYZus{}m}\PY{p}{)}
\PY{n}{a0} \PY{o}{=} \PY{l+m+mf}{1.0}
\PY{n}{rho} \PY{o}{=} \PY{l+m+mf}{0.8}
\PY{n}{c0} \PY{o}{=} \PY{l+m+mf}{0.1}
\PY{n}{NMax} \PY{o}{=} \PY{l+m+mi}{30000}
\PY{n}{KMax} \PY{o}{=} \PY{l+m+mi}{600}
\end{Verbatim}
\end{tcolorbox}

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{49}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{n}{xk}\PY{p}{,} \PY{n}{kiter}\PY{p}{,} \PY{n}{bl}\PY{p}{,} \PY{n}{rcrd} \PY{o}{=} \PY{n}{descmax\PYZus{}bt}\PY{p}{(}\PY{n}{ffun}\PY{p}{,} \PY{n}{gfun}\PY{p}{,} \PY{n}{x0}\PY{p}{,} \PY{n}{tau}\PY{p}{,} \PY{n}{NMax}\PY{p}{,} \PY{n}{a0}\PY{p}{,} \PY{n}{rho}\PY{p}{,} \PY{n}{c0}\PY{p}{,} \PY{n}{KMax}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{50}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{n+nb}{print}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{El numero de iteraciones del algoritmo de descenso fue de: }\PY{l+s+s2}{\PYZdq{}}\PY{p}{,}\PY{n}{kiter}\PY{p}{)}
\PY{n+nb}{print}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{El punto optimo hallado fue: }\PY{l+s+s2}{\PYZdq{}}\PY{p}{,}\PY{n}{xk}\PY{p}{)}
\PY{n+nb}{print}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{El valor de la funcion en el punto optimo hallado es: }\PY{l+s+s2}{\PYZdq{}}\PY{p}{,} \PY{n}{ffun}\PY{p}{(}\PY{n}{xk}\PY{p}{)}\PY{p}{)}
\PY{n+nb}{print}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{La norma del gradiente en el punto optimo hallado es de}\PY{l+s+s2}{\PYZdq{}}\PY{p}{,} \PY{n}{np}\PY{o}{.}\PY{n}{linalg}\PY{o}{.}\PY{n}{norm}\PY{p}{(}\PY{n}{gfun}\PY{p}{(}\PY{n}{xk}\PY{p}{)}\PY{p}{)}\PY{p}{)}
\PY{n+nb}{print}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{El algoritmo termino con condicion de paro: }\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{bl}\PY{p}{)}
\PY{n+nb}{print}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{El valor promedio de los pasos fue: }\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{np}\PY{o}{.}\PY{n}{mean}\PY{p}{(}\PY{n}{rcrd}\PY{p}{[}\PY{p}{:}\PY{p}{,} \PY{l+m+mi}{2}\PY{p}{]}\PY{p}{)}\PY{p}{)}
\PY{n+nb}{print}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{El promedio de iteraciones del algoritmo de backtracking fue de: }\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{np}\PY{o}{.}\PY{n}{mean}\PY{p}{(}\PY{n}{rcrd}\PY{p}{[}\PY{p}{:}\PY{p}{,}\PY{o}{\PYZhy{}}\PY{l+m+mi}{1}\PY{p}{:}\PY{p}{]}\PY{p}{)}\PY{p}{)}

\PY{n}{contornosFnc2D}\PY{p}{(}\PY{n}{ffun}\PY{p}{,} \PY{n}{xsearch}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{]}\PY{p}{,} \PY{n}{xsearch}\PY{p}{[}\PY{l+m+mi}{1}\PY{p}{]}\PY{p}{,} \PY{n}{ysearch}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{]}\PY{p}{,} \PY{n}{ysearch}\PY{p}{[}\PY{l+m+mi}{1}\PY{p}{]}\PY{p}{,} \PY{p}{[}\PY{l+m+mf}{0.5}\PY{p}{,} \PY{l+m+mi}{5}\PY{p}{,} \PY{l+m+mi}{10}\PY{p}{,} \PY{l+m+mi}{25}\PY{p}{,} \PY{l+m+mi}{50}\PY{p}{,} \PY{l+m+mi}{100}\PY{p}{,} \PY{l+m+mi}{150}\PY{p}{,} \PY{l+m+mi}{250}\PY{p}{,} \PY{l+m+mi}{400}\PY{p}{]}\PY{p}{,} \PY{n}{rcrd}\PY{p}{[}\PY{p}{:}\PY{p}{,}\PY{p}{:}\PY{l+m+mi}{2}\PY{p}{]}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

    \begin{Verbatim}[commandchars=\\\{\}]
El numero de iteraciones del algoritmo de descenso fue de:  32
El punto optimo hallado fue:  [2.99999999 1.99999999]
El valor de la funcion en el punto optimo hallado es:  7.151770384664323e-15
La norma del gradiente en el punto optimo hallado es de 1.0439488209969912e-06
El algoritmo termino con condicion de paro:  True
El valor promedio de los pasos fue:  0.023569234228101043
El promedio de iteraciones del algoritmo de backtracking fue de:
17.484848484848484
    \end{Verbatim}

    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{Tarea3_files/Tarea3_17_1.png}
    \end{center}
    { \hspace*{\fill} \\}
    
    \begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

Funcion de Beale

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{51}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{n}{ffun} \PY{o}{=} \PY{n}{bale}
\PY{n}{gfun} \PY{o}{=} \PY{n}{bale\PYZus{}grad}
\PY{n}{xsearch} \PY{o}{=} \PY{p}{[}\PY{o}{\PYZhy{}}\PY{l+m+mf}{4.5}\PY{p}{,}\PY{l+m+mf}{4.5}\PY{p}{]}
\PY{n}{ysearch} \PY{o}{=} \PY{p}{[}\PY{o}{\PYZhy{}}\PY{l+m+mf}{4.5}\PY{p}{,}\PY{l+m+mf}{4.5}\PY{p}{]}
\PY{n}{x0} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{array}\PY{p}{(}\PY{p}{[}\PY{l+m+mf}{2.0}\PY{p}{,} \PY{l+m+mf}{3.0}\PY{p}{]}\PY{p}{)}
\PY{n}{n} \PY{o}{=} \PY{n+nb}{len}\PY{p}{(}\PY{n}{x0}\PY{p}{)}
\PY{n}{tau} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{sqrt}\PY{p}{(}\PY{n}{n}\PY{o}{*}\PY{n}{eps\PYZus{}m}\PY{p}{)}
\PY{n}{a0} \PY{o}{=} \PY{l+m+mf}{1.0}
\PY{n}{rho} \PY{o}{=} \PY{l+m+mf}{0.8}
\PY{n}{c0} \PY{o}{=} \PY{l+m+mf}{0.1}
\PY{n}{NMax} \PY{o}{=} \PY{l+m+mi}{30000}
\PY{n}{KMax} \PY{o}{=} \PY{l+m+mi}{600}
\end{Verbatim}
\end{tcolorbox}

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{52}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{n}{xk}\PY{p}{,} \PY{n}{kiter}\PY{p}{,} \PY{n}{bl}\PY{p}{,} \PY{n}{rcrd} \PY{o}{=} \PY{n}{descmax\PYZus{}bt}\PY{p}{(}\PY{n}{ffun}\PY{p}{,} \PY{n}{gfun}\PY{p}{,} \PY{n}{x0}\PY{p}{,} \PY{n}{tau}\PY{p}{,} \PY{n}{NMax}\PY{p}{,} \PY{n}{a0}\PY{p}{,} \PY{n}{rho}\PY{p}{,} \PY{n}{c0}\PY{p}{,} \PY{n}{KMax}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{53}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{n+nb}{print}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{El numero de iteraciones del algoritmo de descenso fue de: }\PY{l+s+s2}{\PYZdq{}}\PY{p}{,}\PY{n}{kiter}\PY{p}{)}
\PY{n+nb}{print}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{El punto optimo hallado fue: }\PY{l+s+s2}{\PYZdq{}}\PY{p}{,}\PY{n}{xk}\PY{p}{)}
\PY{n+nb}{print}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{El valor de la funcion en el punto optimo hallado es: }\PY{l+s+s2}{\PYZdq{}}\PY{p}{,} \PY{n}{ffun}\PY{p}{(}\PY{n}{xk}\PY{p}{)}\PY{p}{)}
\PY{n+nb}{print}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{La norma del gradiente en el punto optimo hallado es de}\PY{l+s+s2}{\PYZdq{}}\PY{p}{,} \PY{n}{np}\PY{o}{.}\PY{n}{linalg}\PY{o}{.}\PY{n}{norm}\PY{p}{(}\PY{n}{gfun}\PY{p}{(}\PY{n}{xk}\PY{p}{)}\PY{p}{)}\PY{p}{)}
\PY{n+nb}{print}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{El algoritmo termino con condicion de paro: }\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{bl}\PY{p}{)}
\PY{n+nb}{print}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{El valor promedio de los pasos fue: }\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{np}\PY{o}{.}\PY{n}{mean}\PY{p}{(}\PY{n}{rcrd}\PY{p}{[}\PY{p}{:}\PY{p}{,} \PY{l+m+mi}{2}\PY{p}{]}\PY{p}{)}\PY{p}{)}
\PY{n+nb}{print}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{El promedio de iteraciones del algoritmo de backtracking fue de: }\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{np}\PY{o}{.}\PY{n}{mean}\PY{p}{(}\PY{n}{rcrd}\PY{p}{[}\PY{p}{:}\PY{p}{,}\PY{o}{\PYZhy{}}\PY{l+m+mi}{1}\PY{p}{:}\PY{p}{]}\PY{p}{)}\PY{p}{)}

\PY{n}{contornosFnc2D}\PY{p}{(}\PY{n}{ffun}\PY{p}{,} \PY{n}{xsearch}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{]}\PY{p}{,} \PY{n}{xsearch}\PY{p}{[}\PY{l+m+mi}{1}\PY{p}{]}\PY{p}{,} \PY{n}{ysearch}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{]}\PY{p}{,} \PY{n}{ysearch}\PY{p}{[}\PY{l+m+mi}{1}\PY{p}{]}\PY{p}{,} \PY{p}{[}\PY{l+m+mf}{0.5}\PY{p}{,} \PY{l+m+mi}{5}\PY{p}{,} \PY{l+m+mi}{10}\PY{p}{,} \PY{l+m+mi}{25}\PY{p}{,} \PY{l+m+mi}{50}\PY{p}{,} \PY{l+m+mi}{100}\PY{p}{,} \PY{l+m+mi}{150}\PY{p}{,} \PY{l+m+mi}{250}\PY{p}{,} \PY{l+m+mi}{400}\PY{p}{]}\PY{p}{,} \PY{n}{rcrd}\PY{p}{[}\PY{p}{:}\PY{p}{,}\PY{p}{:}\PY{l+m+mi}{2}\PY{p}{]}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

    \begin{Verbatim}[commandchars=\\\{\}]
El numero de iteraciones del algoritmo de descenso fue de:  1011
El punto optimo hallado fue:  [2.99999928 0.49999983]
El valor de la funcion en el punto optimo hallado es:  8.528428776110023e-14
La norma del gradiente en el punto optimo hallado es de 4.667207212327143e-07
El algoritmo termino con condicion de paro:  True
El valor promedio de los pasos fue:  0.04282866564722104
El promedio de iteraciones del algoritmo de backtracking fue de:
14.193675889328063
    \end{Verbatim}

    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{Tarea3_files/Tarea3_21_1.png}
    \end{center}
    { \hspace*{\fill} \\}
    
    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{54}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{n}{ffun} \PY{o}{=} \PY{n}{bale}
\PY{n}{gfun} \PY{o}{=} \PY{n}{bale\PYZus{}grad}
\PY{n}{xsearch} \PY{o}{=} \PY{p}{[}\PY{o}{\PYZhy{}}\PY{l+m+mf}{4.5}\PY{p}{,}\PY{l+m+mi}{8}\PY{p}{]}
\PY{n}{ysearch} \PY{o}{=} \PY{p}{[}\PY{o}{\PYZhy{}}\PY{l+m+mf}{4.5}\PY{p}{,}\PY{l+m+mf}{4.5}\PY{p}{]}
\PY{n}{x0} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{array}\PY{p}{(}\PY{p}{[}\PY{l+m+mf}{2.0}\PY{p}{,} \PY{l+m+mf}{4.0}\PY{p}{]}\PY{p}{)}
\PY{n}{n} \PY{o}{=} \PY{n+nb}{len}\PY{p}{(}\PY{n}{x0}\PY{p}{)}
\PY{n}{tau} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{sqrt}\PY{p}{(}\PY{n}{n}\PY{o}{*}\PY{n}{eps\PYZus{}m}\PY{p}{)}
\PY{n}{a0} \PY{o}{=} \PY{l+m+mf}{1.0}
\PY{n}{rho} \PY{o}{=} \PY{l+m+mf}{0.8}
\PY{n}{c0} \PY{o}{=} \PY{l+m+mf}{0.1}
\PY{n}{NMax} \PY{o}{=} \PY{l+m+mi}{30000}
\PY{n}{KMax} \PY{o}{=} \PY{l+m+mi}{600}
\end{Verbatim}
\end{tcolorbox}

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{55}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{n}{xk}\PY{p}{,} \PY{n}{kiter}\PY{p}{,} \PY{n}{bl}\PY{p}{,} \PY{n}{rcrd} \PY{o}{=} \PY{n}{descmax\PYZus{}bt}\PY{p}{(}\PY{n}{ffun}\PY{p}{,} \PY{n}{gfun}\PY{p}{,} \PY{n}{x0}\PY{p}{,} \PY{n}{tau}\PY{p}{,} \PY{n}{NMax}\PY{p}{,} \PY{n}{a0}\PY{p}{,} \PY{n}{rho}\PY{p}{,} \PY{n}{c0}\PY{p}{,} \PY{n}{KMax}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{56}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{n+nb}{print}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{El numero de iteraciones del algoritmo de descenso fue de: }\PY{l+s+s2}{\PYZdq{}}\PY{p}{,}\PY{n}{kiter}\PY{p}{)}
\PY{n+nb}{print}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{El punto optimo hallado fue: }\PY{l+s+s2}{\PYZdq{}}\PY{p}{,}\PY{n}{xk}\PY{p}{)}
\PY{n+nb}{print}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{El valor de la funcion en el punto optimo hallado es: }\PY{l+s+s2}{\PYZdq{}}\PY{p}{,} \PY{n}{ffun}\PY{p}{(}\PY{n}{xk}\PY{p}{)}\PY{p}{)}
\PY{n+nb}{print}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{La norma del gradiente en el punto optimo hallado es de}\PY{l+s+s2}{\PYZdq{}}\PY{p}{,} \PY{n}{np}\PY{o}{.}\PY{n}{linalg}\PY{o}{.}\PY{n}{norm}\PY{p}{(}\PY{n}{gfun}\PY{p}{(}\PY{n}{xk}\PY{p}{)}\PY{p}{)}\PY{p}{)}
\PY{n+nb}{print}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{El algoritmo termino con condicion de paro: }\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{bl}\PY{p}{)}
\PY{n+nb}{print}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{El valor promedio de los pasos fue: }\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{np}\PY{o}{.}\PY{n}{mean}\PY{p}{(}\PY{n}{rcrd}\PY{p}{[}\PY{p}{:}\PY{p}{,} \PY{l+m+mi}{2}\PY{p}{]}\PY{p}{)}\PY{p}{)}
\PY{n+nb}{print}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{El promedio de iteraciones del algoritmo de backtracking fue de: }\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{np}\PY{o}{.}\PY{n}{mean}\PY{p}{(}\PY{n}{rcrd}\PY{p}{[}\PY{p}{:}\PY{p}{,}\PY{o}{\PYZhy{}}\PY{l+m+mi}{1}\PY{p}{:}\PY{p}{]}\PY{p}{)}\PY{p}{)}

\PY{n}{contornosFnc2D}\PY{p}{(}\PY{n}{ffun}\PY{p}{,} \PY{n}{xsearch}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{]}\PY{p}{,} \PY{n}{xsearch}\PY{p}{[}\PY{l+m+mi}{1}\PY{p}{]}\PY{p}{,} \PY{n}{ysearch}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{]}\PY{p}{,} \PY{n}{ysearch}\PY{p}{[}\PY{l+m+mi}{1}\PY{p}{]}\PY{p}{,} \PY{p}{[}\PY{l+m+mf}{0.5}\PY{p}{,} \PY{l+m+mi}{5}\PY{p}{,} \PY{l+m+mi}{10}\PY{p}{,} \PY{l+m+mi}{25}\PY{p}{,} \PY{l+m+mi}{50}\PY{p}{,} \PY{l+m+mi}{100}\PY{p}{,} \PY{l+m+mi}{150}\PY{p}{,} \PY{l+m+mi}{250}\PY{p}{,} \PY{l+m+mi}{400}\PY{p}{]}\PY{p}{,} \PY{n}{rcrd}\PY{p}{[}\PY{p}{:}\PY{p}{,}\PY{p}{:}\PY{l+m+mi}{2}\PY{p}{]}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

    \begin{Verbatim}[commandchars=\\\{\}]
El numero de iteraciones del algoritmo de descenso fue de:  16864
El punto optimo hallado fue:  [3.00000074 0.50000019]
El valor de la funcion en el punto optimo hallado es:  8.885549089950447e-14
La norma del gradiente en el punto optimo hallado es de 4.7076080101774503e-07
El algoritmo termino con condicion de paro:  True
El valor promedio de los pasos fue:  0.006950835688126628
El promedio de iteraciones del algoritmo de backtracking fue de:
24.097005632967683
    \end{Verbatim}

    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{Tarea3_files/Tarea3_24_1.png}
    \end{center}
    { \hspace*{\fill} \\}
    
    Funcion de Rosenbrock

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{57}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{n}{ffun} \PY{o}{=} \PY{n}{rosenbrock}
\PY{n}{gfun} \PY{o}{=} \PY{n}{rosenbrock\PYZus{}grad}
\PY{n}{xsearch} \PY{o}{=} \PY{p}{[}\PY{o}{\PYZhy{}}\PY{l+m+mi}{3}\PY{p}{,}\PY{l+m+mi}{3}\PY{p}{]}
\PY{n}{ysearch} \PY{o}{=} \PY{p}{[}\PY{o}{\PYZhy{}}\PY{l+m+mi}{1}\PY{p}{,}\PY{l+m+mi}{5}\PY{p}{]}
\PY{n}{x0} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{array}\PY{p}{(}\PY{p}{[}\PY{o}{\PYZhy{}}\PY{l+m+mf}{2.1}\PY{p}{,} \PY{l+m+mf}{4.5}\PY{p}{]}\PY{p}{)}
\PY{n}{n} \PY{o}{=} \PY{n+nb}{len}\PY{p}{(}\PY{n}{x0}\PY{p}{)}
\PY{n}{tau} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{sqrt}\PY{p}{(}\PY{n}{n}\PY{o}{*}\PY{n}{eps\PYZus{}m}\PY{p}{)}
\PY{n}{a0} \PY{o}{=} \PY{l+m+mf}{1.0}
\PY{n}{rho} \PY{o}{=} \PY{l+m+mf}{0.8}
\PY{n}{c0} \PY{o}{=} \PY{l+m+mf}{0.1}
\PY{n}{NMax} \PY{o}{=} \PY{l+m+mi}{30000}
\PY{n}{KMax} \PY{o}{=} \PY{l+m+mi}{600}
\end{Verbatim}
\end{tcolorbox}

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{58}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{n}{xk}\PY{p}{,} \PY{n}{kiter}\PY{p}{,} \PY{n}{bl}\PY{p}{,} \PY{n}{rcrd} \PY{o}{=} \PY{n}{descmax\PYZus{}bt}\PY{p}{(}\PY{n}{ffun}\PY{p}{,} \PY{n}{gfun}\PY{p}{,} \PY{n}{x0}\PY{p}{,} \PY{n}{tau}\PY{p}{,} \PY{n}{NMax}\PY{p}{,} \PY{n}{a0}\PY{p}{,} \PY{n}{rho}\PY{p}{,} \PY{n}{c0}\PY{p}{,} \PY{n}{KMax}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{59}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{n+nb}{print}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{El numero de iteraciones del algoritmo de descenso fue de: }\PY{l+s+s2}{\PYZdq{}}\PY{p}{,}\PY{n}{kiter}\PY{p}{)}
\PY{n+nb}{print}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{El punto optimo hallado fue: }\PY{l+s+s2}{\PYZdq{}}\PY{p}{,}\PY{n}{xk}\PY{p}{)}
\PY{n+nb}{print}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{El valor de la funcion en el punto optimo hallado es: }\PY{l+s+s2}{\PYZdq{}}\PY{p}{,} \PY{n}{ffun}\PY{p}{(}\PY{n}{xk}\PY{p}{)}\PY{p}{)}
\PY{n+nb}{print}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{La norma del gradiente en el punto optimo hallado es de}\PY{l+s+s2}{\PYZdq{}}\PY{p}{,} \PY{n}{np}\PY{o}{.}\PY{n}{linalg}\PY{o}{.}\PY{n}{norm}\PY{p}{(}\PY{n}{gfun}\PY{p}{(}\PY{n}{xk}\PY{p}{)}\PY{p}{)}\PY{p}{)}
\PY{n+nb}{print}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{El algoritmo termino con condicion de paro: }\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{bl}\PY{p}{)}
\PY{n+nb}{print}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{El valor promedio de los pasos fue: }\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{np}\PY{o}{.}\PY{n}{mean}\PY{p}{(}\PY{n}{rcrd}\PY{p}{[}\PY{p}{:}\PY{p}{,} \PY{l+m+mi}{2}\PY{p}{]}\PY{p}{)}\PY{p}{)}
\PY{n+nb}{print}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{El promedio de iteraciones del algoritmo de backtracking fue de: }\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{np}\PY{o}{.}\PY{n}{mean}\PY{p}{(}\PY{n}{rcrd}\PY{p}{[}\PY{p}{:}\PY{p}{,}\PY{o}{\PYZhy{}}\PY{l+m+mi}{1}\PY{p}{:}\PY{p}{]}\PY{p}{)}\PY{p}{)}

\PY{n}{contornosFnc2D}\PY{p}{(}\PY{n}{ffun}\PY{p}{,} \PY{n}{xsearch}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{]}\PY{p}{,} \PY{n}{xsearch}\PY{p}{[}\PY{l+m+mi}{1}\PY{p}{]}\PY{p}{,} \PY{n}{ysearch}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{]}\PY{p}{,} \PY{n}{ysearch}\PY{p}{[}\PY{l+m+mi}{1}\PY{p}{]}\PY{p}{,} \PY{p}{[}\PY{l+m+mf}{0.5}\PY{p}{,} \PY{l+m+mi}{5}\PY{p}{,} \PY{l+m+mi}{10}\PY{p}{,} \PY{l+m+mi}{25}\PY{p}{,} \PY{l+m+mi}{50}\PY{p}{,} \PY{l+m+mi}{100}\PY{p}{,} \PY{l+m+mi}{150}\PY{p}{,} \PY{l+m+mi}{250}\PY{p}{,} \PY{l+m+mi}{400}\PY{p}{]}\PY{p}{,} \PY{n}{rcrd}\PY{p}{[}\PY{p}{:}\PY{p}{,}\PY{p}{:}\PY{l+m+mi}{2}\PY{p}{]}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

    \begin{Verbatim}[commandchars=\\\{\}]
El numero de iteraciones del algoritmo de descenso fue de:  21730
El punto optimo hallado fue:  [1.000006   1.00001204]
El valor de la funcion en el punto optimo hallado es:  3.61526542723387e-11
La norma del gradiente en el punto optimo hallado es de 1.0852939980899932e-05
El algoritmo termino con condicion de paro:  True
El valor promedio de los pasos fue:  0.0016287744602422186
El promedio de iteraciones del algoritmo de backtracking fue de:
29.21494638994984
    \end{Verbatim}

    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{Tarea3_files/Tarea3_28_1.png}
    \end{center}
    { \hspace*{\fill} \\}
    
    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{60}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{n}{ffun} \PY{o}{=} \PY{n}{rosenbrock}
\PY{n}{gfun} \PY{o}{=} \PY{n}{rosenbrock\PYZus{}grad}
\PY{n}{xsearch} \PY{o}{=} \PY{p}{[}\PY{o}{\PYZhy{}}\PY{l+m+mi}{3}\PY{p}{,}\PY{l+m+mi}{3}\PY{p}{]}
\PY{n}{ysearch} \PY{o}{=} \PY{p}{[}\PY{o}{\PYZhy{}}\PY{l+m+mi}{1}\PY{p}{,}\PY{l+m+mi}{5}\PY{p}{]}
\PY{n}{x0} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{array}\PY{p}{(}\PY{p}{[}\PY{o}{\PYZhy{}}\PY{l+m+mf}{1.2}\PY{p}{,} \PY{l+m+mf}{1.0}\PY{p}{]}\PY{p}{)}
\PY{n}{n} \PY{o}{=} \PY{n+nb}{len}\PY{p}{(}\PY{n}{x0}\PY{p}{)}
\PY{n}{tau} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{sqrt}\PY{p}{(}\PY{n}{n}\PY{o}{*}\PY{n}{eps\PYZus{}m}\PY{p}{)}
\PY{n}{a0} \PY{o}{=} \PY{l+m+mf}{1.0}
\PY{n}{rho} \PY{o}{=} \PY{l+m+mf}{0.8}
\PY{n}{c0} \PY{o}{=} \PY{l+m+mf}{0.1}
\PY{n}{NMax} \PY{o}{=} \PY{l+m+mi}{30000}
\PY{n}{KMax} \PY{o}{=} \PY{l+m+mi}{600}
\end{Verbatim}
\end{tcolorbox}

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{61}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{n}{xk}\PY{p}{,} \PY{n}{kiter}\PY{p}{,} \PY{n}{bl}\PY{p}{,} \PY{n}{rcrd} \PY{o}{=} \PY{n}{descmax\PYZus{}bt}\PY{p}{(}\PY{n}{ffun}\PY{p}{,} \PY{n}{gfun}\PY{p}{,} \PY{n}{x0}\PY{p}{,} \PY{n}{tau}\PY{p}{,} \PY{n}{NMax}\PY{p}{,} \PY{n}{a0}\PY{p}{,} \PY{n}{rho}\PY{p}{,} \PY{n}{c0}\PY{p}{,} \PY{n}{KMax}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{62}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{n+nb}{print}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{El numero de iteraciones del algoritmo de descenso fue de: }\PY{l+s+s2}{\PYZdq{}}\PY{p}{,}\PY{n}{kiter}\PY{p}{)}
\PY{n+nb}{print}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{El punto optimo hallado fue: }\PY{l+s+s2}{\PYZdq{}}\PY{p}{,}\PY{n}{xk}\PY{p}{)}
\PY{n+nb}{print}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{El valor de la funcion en el punto optimo hallado es: }\PY{l+s+s2}{\PYZdq{}}\PY{p}{,} \PY{n}{ffun}\PY{p}{(}\PY{n}{xk}\PY{p}{)}\PY{p}{)}
\PY{n+nb}{print}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{La norma del gradiente en el punto optimo hallado es de}\PY{l+s+s2}{\PYZdq{}}\PY{p}{,} \PY{n}{np}\PY{o}{.}\PY{n}{linalg}\PY{o}{.}\PY{n}{norm}\PY{p}{(}\PY{n}{gfun}\PY{p}{(}\PY{n}{xk}\PY{p}{)}\PY{p}{)}\PY{p}{)}
\PY{n+nb}{print}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{El algoritmo termino con condicion de paro: }\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{bl}\PY{p}{)}
\PY{n+nb}{print}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{El valor promedio de los pasos fue: }\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{np}\PY{o}{.}\PY{n}{mean}\PY{p}{(}\PY{n}{rcrd}\PY{p}{[}\PY{p}{:}\PY{p}{,} \PY{l+m+mi}{2}\PY{p}{]}\PY{p}{)}\PY{p}{)}
\PY{n+nb}{print}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{El promedio de iteraciones del algoritmo de backtracking fue de: }\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{np}\PY{o}{.}\PY{n}{mean}\PY{p}{(}\PY{n}{rcrd}\PY{p}{[}\PY{p}{:}\PY{p}{,}\PY{o}{\PYZhy{}}\PY{l+m+mi}{1}\PY{p}{:}\PY{p}{]}\PY{p}{)}\PY{p}{)}

\PY{n}{contornosFnc2D}\PY{p}{(}\PY{n}{ffun}\PY{p}{,} \PY{n}{xsearch}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{]}\PY{p}{,} \PY{n}{xsearch}\PY{p}{[}\PY{l+m+mi}{1}\PY{p}{]}\PY{p}{,} \PY{n}{ysearch}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{]}\PY{p}{,} \PY{n}{ysearch}\PY{p}{[}\PY{l+m+mi}{1}\PY{p}{]}\PY{p}{,} \PY{p}{[}\PY{l+m+mf}{0.5}\PY{p}{,} \PY{l+m+mi}{5}\PY{p}{,} \PY{l+m+mi}{10}\PY{p}{,} \PY{l+m+mi}{25}\PY{p}{,} \PY{l+m+mi}{50}\PY{p}{,} \PY{l+m+mi}{100}\PY{p}{,} \PY{l+m+mi}{150}\PY{p}{,} \PY{l+m+mi}{250}\PY{p}{,} \PY{l+m+mi}{400}\PY{p}{]}\PY{p}{,} \PY{n}{rcrd}\PY{p}{[}\PY{p}{:}\PY{p}{,}\PY{p}{:}\PY{l+m+mi}{2}\PY{p}{]}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

    \begin{Verbatim}[commandchars=\\\{\}]
El numero de iteraciones del algoritmo de descenso fue de:  11166
El punto optimo hallado fue:  [1.00000613 1.00001231]
El valor de la funcion en el punto optimo hallado es:  3.7820775522341363e-11
La norma del gradiente en el punto optimo hallado es de 1.0886691640011656e-05
El algoritmo termino con condicion de paro:  True
El valor promedio de los pasos fue:  0.0020360078098377097
El promedio de iteraciones del algoritmo de backtracking fue de:
27.883316915912957
    \end{Verbatim}

    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{Tarea3_files/Tarea3_31_1.png}
    \end{center}
    { \hspace*{\fill} \\}
    
    \subsubsection{4.- Propuesta de Nocedal}\label{propuesta-de-nocedal}

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{63}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{n}{ffun} \PY{o}{=} \PY{n}{bale}
\PY{n}{gfun} \PY{o}{=} \PY{n}{bale\PYZus{}grad}
\PY{n}{xsearch} \PY{o}{=} \PY{p}{[}\PY{o}{\PYZhy{}}\PY{l+m+mf}{4.5}\PY{p}{,}\PY{l+m+mf}{4.5}\PY{p}{]}
\PY{n}{ysearch} \PY{o}{=} \PY{p}{[}\PY{o}{\PYZhy{}}\PY{l+m+mf}{4.5}\PY{p}{,}\PY{l+m+mf}{4.5}\PY{p}{]}
\PY{n}{x0} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{array}\PY{p}{(}\PY{p}{[}\PY{l+m+mf}{2.0}\PY{p}{,} \PY{l+m+mf}{3.0}\PY{p}{]}\PY{p}{)}
\PY{n}{n} \PY{o}{=} \PY{n+nb}{len}\PY{p}{(}\PY{n}{x0}\PY{p}{)}
\PY{n}{tau} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{sqrt}\PY{p}{(}\PY{n}{n}\PY{o}{*}\PY{n}{eps\PYZus{}m}\PY{p}{)}
\PY{n}{a0} \PY{o}{=} \PY{l+m+mf}{1.0}
\PY{n}{rho} \PY{o}{=} \PY{l+m+mf}{0.8}
\PY{n}{c0} \PY{o}{=} \PY{l+m+mf}{0.0001}
\PY{n}{NMax} \PY{o}{=} \PY{l+m+mi}{30000}
\PY{n}{KMax} \PY{o}{=} \PY{l+m+mi}{600}
\end{Verbatim}
\end{tcolorbox}

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{64}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{n}{xk}\PY{p}{,} \PY{n}{kiter}\PY{p}{,} \PY{n}{bl}\PY{p}{,} \PY{n}{rcrd} \PY{o}{=} \PY{n}{descmax\PYZus{}bt}\PY{p}{(}\PY{n}{ffun}\PY{p}{,} \PY{n}{gfun}\PY{p}{,} \PY{n}{x0}\PY{p}{,} \PY{n}{tau}\PY{p}{,} \PY{n}{NMax}\PY{p}{,} \PY{n}{a0}\PY{p}{,} \PY{n}{rho}\PY{p}{,} \PY{n}{c0}\PY{p}{,} \PY{n}{KMax}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{65}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{n+nb}{print}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{El numero de iteraciones del algoritmo de descenso fue de: }\PY{l+s+s2}{\PYZdq{}}\PY{p}{,}\PY{n}{kiter}\PY{p}{)}
\PY{n+nb}{print}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{El punto optimo hallado fue: }\PY{l+s+s2}{\PYZdq{}}\PY{p}{,}\PY{n}{xk}\PY{p}{)}
\PY{n+nb}{print}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{El valor de la funcion en el punto optimo hallado es: }\PY{l+s+s2}{\PYZdq{}}\PY{p}{,} \PY{n}{ffun}\PY{p}{(}\PY{n}{xk}\PY{p}{)}\PY{p}{)}
\PY{n+nb}{print}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{La norma del gradiente en el punto optimo hallado es de}\PY{l+s+s2}{\PYZdq{}}\PY{p}{,} \PY{n}{np}\PY{o}{.}\PY{n}{linalg}\PY{o}{.}\PY{n}{norm}\PY{p}{(}\PY{n}{gfun}\PY{p}{(}\PY{n}{xk}\PY{p}{)}\PY{p}{)}\PY{p}{)}
\PY{n+nb}{print}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{El algoritmo termino con condicion de paro: }\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{bl}\PY{p}{)}
\PY{n+nb}{print}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{El valor promedio de los pasos fue: }\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{np}\PY{o}{.}\PY{n}{mean}\PY{p}{(}\PY{n}{rcrd}\PY{p}{[}\PY{p}{:}\PY{p}{,} \PY{l+m+mi}{2}\PY{p}{]}\PY{p}{)}\PY{p}{)}
\PY{n+nb}{print}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{El promedio de iteraciones del algoritmo de backtracking fue de: }\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{np}\PY{o}{.}\PY{n}{mean}\PY{p}{(}\PY{n}{rcrd}\PY{p}{[}\PY{p}{:}\PY{p}{,}\PY{o}{\PYZhy{}}\PY{l+m+mi}{1}\PY{p}{:}\PY{p}{]}\PY{p}{)}\PY{p}{)}

\PY{n}{contornosFnc2D}\PY{p}{(}\PY{n}{ffun}\PY{p}{,} \PY{n}{xsearch}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{]}\PY{p}{,} \PY{n}{xsearch}\PY{p}{[}\PY{l+m+mi}{1}\PY{p}{]}\PY{p}{,} \PY{n}{ysearch}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{]}\PY{p}{,} \PY{n}{ysearch}\PY{p}{[}\PY{l+m+mi}{1}\PY{p}{]}\PY{p}{,} \PY{p}{[}\PY{l+m+mf}{0.5}\PY{p}{,} \PY{l+m+mi}{5}\PY{p}{,} \PY{l+m+mi}{10}\PY{p}{,} \PY{l+m+mi}{25}\PY{p}{,} \PY{l+m+mi}{50}\PY{p}{,} \PY{l+m+mi}{100}\PY{p}{,} \PY{l+m+mi}{150}\PY{p}{,} \PY{l+m+mi}{250}\PY{p}{,} \PY{l+m+mi}{400}\PY{p}{]}\PY{p}{,} \PY{n}{rcrd}\PY{p}{[}\PY{p}{:}\PY{p}{,}\PY{p}{:}\PY{l+m+mi}{2}\PY{p}{]}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

    \begin{Verbatim}[commandchars=\\\{\}]
El numero de iteraciones del algoritmo de descenso fue de:  29999
El punto optimo hallado fue:  [-0.00964943  6.72131827]
El valor de la funcion en el punto optimo hallado es:  5.500622340349483
La norma del gradiente en el punto optimo hallado es de 1.131711448244844
El algoritmo termino con condicion de paro:  False
El valor promedio de los pasos fue:  1.6435986478438556e-05
El promedio de iteraciones del algoritmo de backtracking fue de:  51.4287
    \end{Verbatim}

    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{Tarea3_files/Tarea3_35_1.png}
    \end{center}
    { \hspace*{\fill} \\}
    
    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{66}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{n}{ffun} \PY{o}{=} \PY{n}{bale}
\PY{n}{gfun} \PY{o}{=} \PY{n}{bale\PYZus{}grad}
\PY{n}{xsearch} \PY{o}{=} \PY{p}{[}\PY{o}{\PYZhy{}}\PY{l+m+mf}{4.5}\PY{p}{,}\PY{l+m+mf}{4.5}\PY{p}{]}
\PY{n}{ysearch} \PY{o}{=} \PY{p}{[}\PY{o}{\PYZhy{}}\PY{l+m+mf}{4.5}\PY{p}{,}\PY{l+m+mf}{4.5}\PY{p}{]}
\PY{n}{x0} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{array}\PY{p}{(}\PY{p}{[}\PY{l+m+mf}{2.0}\PY{p}{,} \PY{l+m+mf}{4.0}\PY{p}{]}\PY{p}{)}
\PY{n}{n} \PY{o}{=} \PY{n+nb}{len}\PY{p}{(}\PY{n}{x0}\PY{p}{)}
\PY{n}{tau} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{sqrt}\PY{p}{(}\PY{n}{n}\PY{o}{*}\PY{n}{eps\PYZus{}m}\PY{p}{)}
\PY{n}{a0} \PY{o}{=} \PY{l+m+mf}{1.0}
\PY{n}{rho} \PY{o}{=} \PY{l+m+mf}{0.8}
\PY{n}{c0} \PY{o}{=} \PY{l+m+mf}{0.0001}
\PY{n}{NMax} \PY{o}{=} \PY{l+m+mi}{30000}
\PY{n}{KMax} \PY{o}{=} \PY{l+m+mi}{600}
\end{Verbatim}
\end{tcolorbox}

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{67}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{n}{xk}\PY{p}{,} \PY{n}{kiter}\PY{p}{,} \PY{n}{bl}\PY{p}{,} \PY{n}{rcrd} \PY{o}{=} \PY{n}{descmax\PYZus{}bt}\PY{p}{(}\PY{n}{ffun}\PY{p}{,} \PY{n}{gfun}\PY{p}{,} \PY{n}{x0}\PY{p}{,} \PY{n}{tau}\PY{p}{,} \PY{n}{NMax}\PY{p}{,} \PY{n}{a0}\PY{p}{,} \PY{n}{rho}\PY{p}{,} \PY{n}{c0}\PY{p}{,} \PY{n}{KMax}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{68}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{n+nb}{print}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{El numero de iteraciones del algoritmo de descenso fue de: }\PY{l+s+s2}{\PYZdq{}}\PY{p}{,}\PY{n}{kiter}\PY{p}{)}
\PY{n+nb}{print}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{El punto optimo hallado fue: }\PY{l+s+s2}{\PYZdq{}}\PY{p}{,}\PY{n}{xk}\PY{p}{)}
\PY{n+nb}{print}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{El valor de la funcion en el punto optimo hallado es: }\PY{l+s+s2}{\PYZdq{}}\PY{p}{,} \PY{n}{ffun}\PY{p}{(}\PY{n}{xk}\PY{p}{)}\PY{p}{)}
\PY{n+nb}{print}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{La norma del gradiente en el punto optimo hallado es de}\PY{l+s+s2}{\PYZdq{}}\PY{p}{,} \PY{n}{np}\PY{o}{.}\PY{n}{linalg}\PY{o}{.}\PY{n}{norm}\PY{p}{(}\PY{n}{gfun}\PY{p}{(}\PY{n}{xk}\PY{p}{)}\PY{p}{)}\PY{p}{)}
\PY{n+nb}{print}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{El algoritmo termino con condicion de paro: }\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{bl}\PY{p}{)}
\PY{n+nb}{print}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{El valor promedio de los pasos fue: }\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{np}\PY{o}{.}\PY{n}{mean}\PY{p}{(}\PY{n}{rcrd}\PY{p}{[}\PY{p}{:}\PY{p}{,} \PY{l+m+mi}{2}\PY{p}{]}\PY{p}{)}\PY{p}{)}
\PY{n+nb}{print}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{El promedio de iteraciones del algoritmo de backtracking fue de: }\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{np}\PY{o}{.}\PY{n}{mean}\PY{p}{(}\PY{n}{rcrd}\PY{p}{[}\PY{p}{:}\PY{p}{,}\PY{o}{\PYZhy{}}\PY{l+m+mi}{1}\PY{p}{:}\PY{p}{]}\PY{p}{)}\PY{p}{)}

\PY{n}{contornosFnc2D}\PY{p}{(}\PY{n}{ffun}\PY{p}{,} \PY{n}{xsearch}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{]}\PY{p}{,} \PY{n}{xsearch}\PY{p}{[}\PY{l+m+mi}{1}\PY{p}{]}\PY{p}{,} \PY{n}{ysearch}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{]}\PY{p}{,} \PY{n}{ysearch}\PY{p}{[}\PY{l+m+mi}{1}\PY{p}{]}\PY{p}{,} \PY{p}{[}\PY{l+m+mf}{0.5}\PY{p}{,} \PY{l+m+mi}{5}\PY{p}{,} \PY{l+m+mi}{10}\PY{p}{,} \PY{l+m+mi}{25}\PY{p}{,} \PY{l+m+mi}{50}\PY{p}{,} \PY{l+m+mi}{100}\PY{p}{,} \PY{l+m+mi}{150}\PY{p}{,} \PY{l+m+mi}{250}\PY{p}{,} \PY{l+m+mi}{400}\PY{p}{]}\PY{p}{,} \PY{n}{rcrd}\PY{p}{[}\PY{p}{:}\PY{p}{,}\PY{p}{:}\PY{l+m+mi}{2}\PY{p}{]}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

    \begin{Verbatim}[commandchars=\\\{\}]
El numero de iteraciones del algoritmo de descenso fue de:  10823
El punto optimo hallado fue:  [3.00000046 0.50000012]
El valor de la funcion en el punto optimo hallado es:  3.605743478619451e-14
La norma del gradiente en el punto optimo hallado es de 4.619172961344461e-07
El algoritmo termino con condicion de paro:  True
El valor promedio de los pasos fue:  0.009397115545220469
El promedio de iteraciones del algoritmo de backtracking fue de:
22.937453806356245
    \end{Verbatim}

    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{Tarea3_files/Tarea3_38_1.png}
    \end{center}
    { \hspace*{\fill} \\}
    
    \subsubsection{Comentario}\label{comentario}

Bien en este caso, parece ser que en uno de los casos empeoro el
resultado a tal punto de que no se pudo alcanzar la convergencia,
aumentando tambien el promedio de iteraciones del algoritmo de
backtracking casi 5 veces; sin embargo, para el segundo caso, es posible
observar que el numero de iteraciones del algortimo de descenso decrecio
de forma sustancial y, aunque no de manera notoria, tambien el promedio
de las iteraciones del algoritmo de backtracking se vio favorecido.

La diferencia entre estos es el punto inicial, ya que la funcion de
Beale evaluada en cada punto inicial cambia mientras que para el caso
donde se empeoro el valor inicial es mas pequenio en al menos un orden
de magnitud que para el caso que se mejoro.

    \begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\subsection{Ejercicio 2}\label{ejercicio-2}

    \subsubsection{1.- Reprogramando la
funcion}\label{reprogramando-la-funcion}

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{69}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{k}{def} \PY{n+nf}{descmax\PYZus{}bt\PYZus{}exact}\PY{p}{(}\PY{n}{f\PYZus{}fun}\PY{p}{,} \PY{n}{grad\PYZus{}fun}\PY{p}{,} \PY{n}{x0}\PY{p}{,} \PY{n}{tau}\PY{p}{,} \PY{n}{NMax}\PY{p}{,} \PY{n}{a0}\PY{p}{,} \PY{n}{rho}\PY{p}{,} \PY{n}{c0}\PY{p}{,} \PY{n}{KMax}\PY{p}{)}\PY{p}{:}
    \PY{n}{record} \PY{o}{=} \PY{n+nb}{list}\PY{p}{(}\PY{p}{)}
    \PY{k}{for} \PY{n}{k} \PY{o+ow}{in} \PY{n+nb}{range}\PY{p}{(}\PY{n}{NMax}\PY{p}{)}\PY{p}{:}
        \PY{n}{g} \PY{o}{=} \PY{n}{grad\PYZus{}fun}\PY{p}{(}\PY{n}{x0}\PY{p}{)}
        \PY{n}{p} \PY{o}{=} \PY{o}{\PYZhy{}}\PY{n}{g}
        \PY{k}{if} \PY{n}{k}\PY{o}{\PYZgt{}}\PY{l+m+mi}{0}\PY{p}{:}
            \PY{n}{a0} \PY{o}{=} \PY{n}{a\PYZus{}v}\PY{o}{*} \PY{p}{(}\PY{n}{np}\PY{o}{.}\PY{n}{linalg}\PY{o}{.}\PY{n}{norm}\PY{p}{(}\PY{n}{g\PYZus{}v}\PY{p}{)} \PY{o}{/} \PY{n}{np}\PY{o}{.}\PY{n}{linalg}\PY{o}{.}\PY{n}{norm}\PY{p}{(}\PY{n}{g}\PY{p}{)}\PY{p}{)}
        \PY{n}{a}\PY{p}{,} \PY{n}{i} \PY{o}{=} \PY{n}{bt\PYZus{}armijo}\PY{p}{(}\PY{n}{a0}\PY{p}{,} \PY{n}{rho}\PY{p}{,} \PY{n}{c0}\PY{p}{,} \PY{n}{x0}\PY{p}{,} \PY{n}{f\PYZus{}fun}\PY{p}{,} \PY{n}{grad\PYZus{}fun}\PY{p}{,} \PY{n}{p}\PY{p}{,} \PY{n}{KMax}\PY{p}{)}
        \PY{k}{if} \PY{n+nb}{len}\PY{p}{(}\PY{n}{x0}\PY{p}{)} \PY{o}{==} \PY{l+m+mi}{2}\PY{p}{:}
            \PY{n}{record}\PY{o}{.}\PY{n}{append}\PY{p}{(}\PY{p}{[}\PY{n}{x0}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{]}\PY{p}{,} \PY{n}{x0}\PY{p}{[}\PY{l+m+mi}{1}\PY{p}{]}\PY{p}{,} \PY{n}{a}\PY{p}{,} \PY{n}{i}\PY{p}{]}\PY{p}{)}
        \PY{k}{if} \PY{n}{a} \PY{o}{*} \PY{n}{np}\PY{o}{.}\PY{n}{linalg}\PY{o}{.}\PY{n}{norm}\PY{p}{(}\PY{n}{p}\PY{p}{)} \PY{o}{\PYZlt{}} \PY{n}{tau}\PY{p}{:}
            \PY{k}{return} \PY{n}{x0}\PY{p}{,} \PY{n}{k}\PY{p}{,} \PY{k+kc}{True}\PY{p}{,} \PY{n}{np}\PY{o}{.}\PY{n}{array}\PY{p}{(}\PY{n}{record}\PY{p}{)}
        \PY{n}{x0} \PY{o}{+}\PY{o}{=} \PY{n}{a}\PY{o}{*}\PY{n}{p}
        \PY{n}{a\PYZus{}v} \PY{o}{=} \PY{n}{a}
        \PY{n}{g\PYZus{}v} \PY{o}{=} \PY{n}{g}
    \PY{k}{return} \PY{n}{x0}\PY{p}{,} \PY{n}{k}\PY{p}{,} \PY{k+kc}{False}\PY{p}{,} \PY{n}{np}\PY{o}{.}\PY{n}{array}\PY{p}{(}\PY{n}{record}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

    \subsubsection{Repitiendo las pruebas}\label{repitiendo-las-pruebas}

    Funcion de Himmelblau

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{70}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{n}{ffun} \PY{o}{=} \PY{n}{himmelblau}
\PY{n}{gfun} \PY{o}{=} \PY{n}{himmelblau\PYZus{}grad}
\PY{n}{xsearch} \PY{o}{=} \PY{p}{[}\PY{o}{\PYZhy{}}\PY{l+m+mi}{5}\PY{p}{,}\PY{l+m+mi}{5}\PY{p}{]}
\PY{n}{ysearch} \PY{o}{=} \PY{p}{[}\PY{o}{\PYZhy{}}\PY{l+m+mi}{5}\PY{p}{,}\PY{l+m+mi}{5}\PY{p}{]}
\PY{n}{x0} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{array}\PY{p}{(}\PY{p}{[}\PY{l+m+mf}{2.0}\PY{p}{,} \PY{l+m+mf}{4.0}\PY{p}{]}\PY{p}{)}
\PY{n}{n} \PY{o}{=} \PY{n+nb}{len}\PY{p}{(}\PY{n}{x0}\PY{p}{)}
\PY{n}{tau} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{sqrt}\PY{p}{(}\PY{n}{n}\PY{o}{*}\PY{n}{eps\PYZus{}m}\PY{p}{)}
\PY{n}{a0} \PY{o}{=} \PY{l+m+mf}{1.0}
\PY{n}{rho} \PY{o}{=} \PY{l+m+mf}{0.8}
\PY{n}{c0} \PY{o}{=} \PY{l+m+mf}{0.1}
\PY{n}{NMax} \PY{o}{=} \PY{l+m+mi}{30000}
\PY{n}{KMax} \PY{o}{=} \PY{l+m+mi}{600}
\PY{n}{xk}\PY{p}{,} \PY{n}{kiter}\PY{p}{,} \PY{n}{bl}\PY{p}{,} \PY{n}{rcrd} \PY{o}{=} \PY{n}{descmax\PYZus{}bt\PYZus{}exact}\PY{p}{(}\PY{n}{ffun}\PY{p}{,} \PY{n}{gfun}\PY{p}{,} \PY{n}{x0}\PY{p}{,} \PY{n}{tau}\PY{p}{,} \PY{n}{NMax}\PY{p}{,} \PY{n}{a0}\PY{p}{,} \PY{n}{rho}\PY{p}{,} \PY{n}{c0}\PY{p}{,} \PY{n}{KMax}\PY{p}{)}
\PY{n+nb}{print}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{El numero de iteraciones del algoritmo de descenso fue de: }\PY{l+s+s2}{\PYZdq{}}\PY{p}{,}\PY{n}{kiter}\PY{p}{)}
\PY{n+nb}{print}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{El punto optimo hallado fue: }\PY{l+s+s2}{\PYZdq{}}\PY{p}{,}\PY{n}{xk}\PY{p}{)}
\PY{n+nb}{print}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{El valor de la funcion en el punto optimo hallado es: }\PY{l+s+s2}{\PYZdq{}}\PY{p}{,} \PY{n}{ffun}\PY{p}{(}\PY{n}{xk}\PY{p}{)}\PY{p}{)}
\PY{n+nb}{print}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{La norma del gradiente en el punto optimo hallado es de}\PY{l+s+s2}{\PYZdq{}}\PY{p}{,} \PY{n}{np}\PY{o}{.}\PY{n}{linalg}\PY{o}{.}\PY{n}{norm}\PY{p}{(}\PY{n}{gfun}\PY{p}{(}\PY{n}{xk}\PY{p}{)}\PY{p}{)}\PY{p}{)}
\PY{n+nb}{print}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{El algoritmo termino con condicion de paro: }\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{bl}\PY{p}{)}
\PY{n+nb}{print}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{El valor promedio de los pasos fue: }\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{np}\PY{o}{.}\PY{n}{mean}\PY{p}{(}\PY{n}{rcrd}\PY{p}{[}\PY{p}{:}\PY{p}{,} \PY{l+m+mi}{2}\PY{p}{]}\PY{p}{)}\PY{p}{)}
\PY{n+nb}{print}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{El promedio de iteraciones del algoritmo de backtracking fue de: }\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{np}\PY{o}{.}\PY{n}{mean}\PY{p}{(}\PY{n}{rcrd}\PY{p}{[}\PY{p}{:}\PY{p}{,}\PY{o}{\PYZhy{}}\PY{l+m+mi}{1}\PY{p}{:}\PY{p}{]}\PY{p}{)}\PY{p}{)}

\PY{n}{contornosFnc2D}\PY{p}{(}\PY{n}{ffun}\PY{p}{,} \PY{n}{xsearch}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{]}\PY{p}{,} \PY{n}{xsearch}\PY{p}{[}\PY{l+m+mi}{1}\PY{p}{]}\PY{p}{,} \PY{n}{ysearch}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{]}\PY{p}{,} \PY{n}{ysearch}\PY{p}{[}\PY{l+m+mi}{1}\PY{p}{]}\PY{p}{,} \PY{p}{[}\PY{l+m+mf}{0.5}\PY{p}{,} \PY{l+m+mi}{5}\PY{p}{,} \PY{l+m+mi}{10}\PY{p}{,} \PY{l+m+mi}{25}\PY{p}{,} \PY{l+m+mi}{50}\PY{p}{,} \PY{l+m+mi}{100}\PY{p}{,} \PY{l+m+mi}{150}\PY{p}{,} \PY{l+m+mi}{250}\PY{p}{,} \PY{l+m+mi}{400}\PY{p}{]}\PY{p}{,} \PY{n}{rcrd}\PY{p}{[}\PY{p}{:}\PY{p}{,}\PY{p}{:}\PY{l+m+mi}{2}\PY{p}{]}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

    \begin{Verbatim}[commandchars=\\\{\}]
El numero de iteraciones del algoritmo de descenso fue de:  38
El punto optimo hallado fue:  [3.00000001 2.        ]
El valor de la funcion en el punto optimo hallado es:  2.961213326725033e-15
La norma del gradiente en el punto optimo hallado es de 6.980849248515646e-07
El algoritmo termino con condicion de paro:  True
El valor promedio de los pasos fue:  0.019762102632585873
El promedio de iteraciones del algoritmo de backtracking fue de:
2.6666666666666665
    \end{Verbatim}

    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{Tarea3_files/Tarea3_45_1.png}
    \end{center}
    { \hspace*{\fill} \\}
    
    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{71}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{n}{ffun} \PY{o}{=} \PY{n}{himmelblau}
\PY{n}{gfun} \PY{o}{=} \PY{n}{himmelblau\PYZus{}grad}
\PY{n}{xsearch} \PY{o}{=} \PY{p}{[}\PY{o}{\PYZhy{}}\PY{l+m+mi}{5}\PY{p}{,}\PY{l+m+mi}{5}\PY{p}{]}
\PY{n}{ysearch} \PY{o}{=} \PY{p}{[}\PY{o}{\PYZhy{}}\PY{l+m+mi}{5}\PY{p}{,}\PY{l+m+mi}{5}\PY{p}{]}
\PY{n}{x0} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{array}\PY{p}{(}\PY{p}{[}\PY{l+m+mf}{0.0}\PY{p}{,} \PY{l+m+mf}{0.0}\PY{p}{]}\PY{p}{)}
\PY{n}{n} \PY{o}{=} \PY{n+nb}{len}\PY{p}{(}\PY{n}{x0}\PY{p}{)}
\PY{n}{tau} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{sqrt}\PY{p}{(}\PY{n}{n}\PY{o}{*}\PY{n}{eps\PYZus{}m}\PY{p}{)}
\PY{n}{a0} \PY{o}{=} \PY{l+m+mf}{1.0}
\PY{n}{rho} \PY{o}{=} \PY{l+m+mf}{0.8}
\PY{n}{c0} \PY{o}{=} \PY{l+m+mf}{0.1}
\PY{n}{NMax} \PY{o}{=} \PY{l+m+mi}{30000}
\PY{n}{KMax} \PY{o}{=} \PY{l+m+mi}{600}
\PY{n}{xk}\PY{p}{,} \PY{n}{kiter}\PY{p}{,} \PY{n}{bl}\PY{p}{,} \PY{n}{rcrd} \PY{o}{=} \PY{n}{descmax\PYZus{}bt\PYZus{}exact}\PY{p}{(}\PY{n}{ffun}\PY{p}{,} \PY{n}{gfun}\PY{p}{,} \PY{n}{x0}\PY{p}{,} \PY{n}{tau}\PY{p}{,} \PY{n}{NMax}\PY{p}{,} \PY{n}{a0}\PY{p}{,} \PY{n}{rho}\PY{p}{,} \PY{n}{c0}\PY{p}{,} \PY{n}{KMax}\PY{p}{)}
\PY{n+nb}{print}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{El numero de iteraciones del algoritmo de descenso fue de: }\PY{l+s+s2}{\PYZdq{}}\PY{p}{,}\PY{n}{kiter}\PY{p}{)}
\PY{n+nb}{print}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{El punto optimo hallado fue: }\PY{l+s+s2}{\PYZdq{}}\PY{p}{,}\PY{n}{xk}\PY{p}{)}
\PY{n+nb}{print}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{El valor de la funcion en el punto optimo hallado es: }\PY{l+s+s2}{\PYZdq{}}\PY{p}{,} \PY{n}{ffun}\PY{p}{(}\PY{n}{xk}\PY{p}{)}\PY{p}{)}
\PY{n+nb}{print}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{La norma del gradiente en el punto optimo hallado es de}\PY{l+s+s2}{\PYZdq{}}\PY{p}{,} \PY{n}{np}\PY{o}{.}\PY{n}{linalg}\PY{o}{.}\PY{n}{norm}\PY{p}{(}\PY{n}{gfun}\PY{p}{(}\PY{n}{xk}\PY{p}{)}\PY{p}{)}\PY{p}{)}
\PY{n+nb}{print}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{El algoritmo termino con condicion de paro: }\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{bl}\PY{p}{)}
\PY{n+nb}{print}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{El valor promedio de los pasos fue: }\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{np}\PY{o}{.}\PY{n}{mean}\PY{p}{(}\PY{n}{rcrd}\PY{p}{[}\PY{p}{:}\PY{p}{,} \PY{l+m+mi}{2}\PY{p}{]}\PY{p}{)}\PY{p}{)}
\PY{n+nb}{print}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{El promedio de iteraciones del algoritmo de backtracking fue de: }\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{np}\PY{o}{.}\PY{n}{mean}\PY{p}{(}\PY{n}{rcrd}\PY{p}{[}\PY{p}{:}\PY{p}{,}\PY{o}{\PYZhy{}}\PY{l+m+mi}{1}\PY{p}{:}\PY{p}{]}\PY{p}{)}\PY{p}{)}

\PY{n}{contornosFnc2D}\PY{p}{(}\PY{n}{ffun}\PY{p}{,} \PY{n}{xsearch}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{]}\PY{p}{,} \PY{n}{xsearch}\PY{p}{[}\PY{l+m+mi}{1}\PY{p}{]}\PY{p}{,} \PY{n}{ysearch}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{]}\PY{p}{,} \PY{n}{ysearch}\PY{p}{[}\PY{l+m+mi}{1}\PY{p}{]}\PY{p}{,} \PY{p}{[}\PY{l+m+mf}{0.5}\PY{p}{,} \PY{l+m+mi}{5}\PY{p}{,} \PY{l+m+mi}{10}\PY{p}{,} \PY{l+m+mi}{25}\PY{p}{,} \PY{l+m+mi}{50}\PY{p}{,} \PY{l+m+mi}{100}\PY{p}{,} \PY{l+m+mi}{150}\PY{p}{,} \PY{l+m+mi}{250}\PY{p}{,} \PY{l+m+mi}{400}\PY{p}{]}\PY{p}{,} \PY{n}{rcrd}\PY{p}{[}\PY{p}{:}\PY{p}{,}\PY{p}{:}\PY{l+m+mi}{2}\PY{p}{]}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

    \begin{Verbatim}[commandchars=\\\{\}]
El numero de iteraciones del algoritmo de descenso fue de:  37
El punto optimo hallado fue:  [3.00000001 2.        ]
El valor de la funcion en el punto optimo hallado es:  2.823676151945246e-15
La norma del gradiente en el punto optimo hallado es de 6.816747434228448e-07
El algoritmo termino con condicion de paro:  True
El valor promedio de los pasos fue:  0.023434213890080643
El promedio de iteraciones del algoritmo de backtracking fue de:
2.526315789473684
    \end{Verbatim}

    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{Tarea3_files/Tarea3_46_1.png}
    \end{center}
    { \hspace*{\fill} \\}
    
    Funcion de Beale

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{72}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{n}{ffun} \PY{o}{=} \PY{n}{bale}
\PY{n}{gfun} \PY{o}{=} \PY{n}{bale\PYZus{}grad}
\PY{n}{xsearch} \PY{o}{=} \PY{p}{[}\PY{o}{\PYZhy{}}\PY{l+m+mf}{4.5}\PY{p}{,}\PY{l+m+mf}{4.5}\PY{p}{]}
\PY{n}{ysearch} \PY{o}{=} \PY{p}{[}\PY{o}{\PYZhy{}}\PY{l+m+mf}{4.5}\PY{p}{,}\PY{l+m+mf}{4.5}\PY{p}{]}
\PY{n}{x0} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{array}\PY{p}{(}\PY{p}{[}\PY{l+m+mf}{2.0}\PY{p}{,} \PY{l+m+mf}{3.0}\PY{p}{]}\PY{p}{)}
\PY{n}{n} \PY{o}{=} \PY{n+nb}{len}\PY{p}{(}\PY{n}{x0}\PY{p}{)}
\PY{n}{tau} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{sqrt}\PY{p}{(}\PY{n}{n}\PY{o}{*}\PY{n}{eps\PYZus{}m}\PY{p}{)}
\PY{n}{a0} \PY{o}{=} \PY{l+m+mf}{1.0}
\PY{n}{rho} \PY{o}{=} \PY{l+m+mf}{0.8}
\PY{n}{c0} \PY{o}{=} \PY{l+m+mf}{0.1}
\PY{n}{NMax} \PY{o}{=} \PY{l+m+mi}{30000}
\PY{n}{KMax} \PY{o}{=} \PY{l+m+mi}{600}
\PY{n}{xk}\PY{p}{,} \PY{n}{kiter}\PY{p}{,} \PY{n}{bl}\PY{p}{,} \PY{n}{rcrd} \PY{o}{=} \PY{n}{descmax\PYZus{}bt\PYZus{}exact}\PY{p}{(}\PY{n}{ffun}\PY{p}{,} \PY{n}{gfun}\PY{p}{,} \PY{n}{x0}\PY{p}{,} \PY{n}{tau}\PY{p}{,} \PY{n}{NMax}\PY{p}{,} \PY{n}{a0}\PY{p}{,} \PY{n}{rho}\PY{p}{,} \PY{n}{c0}\PY{p}{,} \PY{n}{KMax}\PY{p}{)}
\PY{n+nb}{print}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{El numero de iteraciones del algoritmo de descenso fue de: }\PY{l+s+s2}{\PYZdq{}}\PY{p}{,}\PY{n}{kiter}\PY{p}{)}
\PY{n+nb}{print}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{El punto optimo hallado fue: }\PY{l+s+s2}{\PYZdq{}}\PY{p}{,}\PY{n}{xk}\PY{p}{)}
\PY{n+nb}{print}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{El valor de la funcion en el punto optimo hallado es: }\PY{l+s+s2}{\PYZdq{}}\PY{p}{,} \PY{n}{ffun}\PY{p}{(}\PY{n}{xk}\PY{p}{)}\PY{p}{)}
\PY{n+nb}{print}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{La norma del gradiente en el punto optimo hallado es de}\PY{l+s+s2}{\PYZdq{}}\PY{p}{,} \PY{n}{np}\PY{o}{.}\PY{n}{linalg}\PY{o}{.}\PY{n}{norm}\PY{p}{(}\PY{n}{gfun}\PY{p}{(}\PY{n}{xk}\PY{p}{)}\PY{p}{)}\PY{p}{)}
\PY{n+nb}{print}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{El algoritmo termino con condicion de paro: }\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{bl}\PY{p}{)}
\PY{n+nb}{print}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{El valor promedio de los pasos fue: }\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{np}\PY{o}{.}\PY{n}{mean}\PY{p}{(}\PY{n}{rcrd}\PY{p}{[}\PY{p}{:}\PY{p}{,} \PY{l+m+mi}{2}\PY{p}{]}\PY{p}{)}\PY{p}{)}
\PY{n+nb}{print}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{El promedio de iteraciones del algoritmo de backtracking fue de: }\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{np}\PY{o}{.}\PY{n}{mean}\PY{p}{(}\PY{n}{rcrd}\PY{p}{[}\PY{p}{:}\PY{p}{,}\PY{o}{\PYZhy{}}\PY{l+m+mi}{1}\PY{p}{:}\PY{p}{]}\PY{p}{)}\PY{p}{)}

\PY{n}{contornosFnc2D}\PY{p}{(}\PY{n}{ffun}\PY{p}{,} \PY{n}{xsearch}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{]}\PY{p}{,} \PY{n}{xsearch}\PY{p}{[}\PY{l+m+mi}{1}\PY{p}{]}\PY{p}{,} \PY{n}{ysearch}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{]}\PY{p}{,} \PY{n}{ysearch}\PY{p}{[}\PY{l+m+mi}{1}\PY{p}{]}\PY{p}{,} \PY{p}{[}\PY{l+m+mf}{0.5}\PY{p}{,} \PY{l+m+mi}{5}\PY{p}{,} \PY{l+m+mi}{10}\PY{p}{,} \PY{l+m+mi}{25}\PY{p}{,} \PY{l+m+mi}{50}\PY{p}{,} \PY{l+m+mi}{100}\PY{p}{,} \PY{l+m+mi}{150}\PY{p}{,} \PY{l+m+mi}{250}\PY{p}{,} \PY{l+m+mi}{400}\PY{p}{]}\PY{p}{,} \PY{n}{rcrd}\PY{p}{[}\PY{p}{:}\PY{p}{,}\PY{p}{:}\PY{l+m+mi}{2}\PY{p}{]}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

    \begin{Verbatim}[commandchars=\\\{\}]
El numero de iteraciones del algoritmo de descenso fue de:  1028
El punto optimo hallado fue:  [2.99999935 0.49999983]
El valor de la funcion en el punto optimo hallado es:  7.177708227617135e-14
La norma del gradiente en el punto optimo hallado es de 6.37517508891056e-07
El algoritmo termino con condicion de paro:  True
El valor promedio de los pasos fue:  0.04244873581954145
El promedio de iteraciones del algoritmo de backtracking fue de:
0.11564625850340136
    \end{Verbatim}

    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{Tarea3_files/Tarea3_48_1.png}
    \end{center}
    { \hspace*{\fill} \\}
    
    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{73}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{n}{ffun} \PY{o}{=} \PY{n}{bale}
\PY{n}{gfun} \PY{o}{=} \PY{n}{bale\PYZus{}grad}
\PY{n}{xsearch} \PY{o}{=} \PY{p}{[}\PY{o}{\PYZhy{}}\PY{l+m+mf}{4.5}\PY{p}{,}\PY{l+m+mf}{4.5}\PY{p}{]}
\PY{n}{ysearch} \PY{o}{=} \PY{p}{[}\PY{o}{\PYZhy{}}\PY{l+m+mf}{4.5}\PY{p}{,}\PY{l+m+mf}{4.5}\PY{p}{]}
\PY{n}{x0} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{array}\PY{p}{(}\PY{p}{[}\PY{l+m+mf}{2.0}\PY{p}{,} \PY{l+m+mf}{4.0}\PY{p}{]}\PY{p}{)}
\PY{n}{n} \PY{o}{=} \PY{n+nb}{len}\PY{p}{(}\PY{n}{x0}\PY{p}{)}
\PY{n}{tau} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{sqrt}\PY{p}{(}\PY{n}{n}\PY{o}{*}\PY{n}{eps\PYZus{}m}\PY{p}{)}
\PY{n}{a0} \PY{o}{=} \PY{l+m+mf}{1.0}
\PY{n}{rho} \PY{o}{=} \PY{l+m+mf}{0.8}
\PY{n}{c0} \PY{o}{=} \PY{l+m+mf}{0.1}
\PY{n}{NMax} \PY{o}{=} \PY{l+m+mi}{30000}
\PY{n}{KMax} \PY{o}{=} \PY{l+m+mi}{600}
\PY{n}{xk}\PY{p}{,} \PY{n}{kiter}\PY{p}{,} \PY{n}{bl}\PY{p}{,} \PY{n}{rcrd} \PY{o}{=} \PY{n}{descmax\PYZus{}bt\PYZus{}exact}\PY{p}{(}\PY{n}{ffun}\PY{p}{,} \PY{n}{gfun}\PY{p}{,} \PY{n}{x0}\PY{p}{,} \PY{n}{tau}\PY{p}{,} \PY{n}{NMax}\PY{p}{,} \PY{n}{a0}\PY{p}{,} \PY{n}{rho}\PY{p}{,} \PY{n}{c0}\PY{p}{,} \PY{n}{KMax}\PY{p}{)}
\PY{n+nb}{print}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{El numero de iteraciones del algoritmo de descenso fue de: }\PY{l+s+s2}{\PYZdq{}}\PY{p}{,}\PY{n}{kiter}\PY{p}{)}
\PY{n+nb}{print}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{El punto optimo hallado fue: }\PY{l+s+s2}{\PYZdq{}}\PY{p}{,}\PY{n}{xk}\PY{p}{)}
\PY{n+nb}{print}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{El valor de la funcion en el punto optimo hallado es: }\PY{l+s+s2}{\PYZdq{}}\PY{p}{,} \PY{n}{ffun}\PY{p}{(}\PY{n}{xk}\PY{p}{)}\PY{p}{)}
\PY{n+nb}{print}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{La norma del gradiente en el punto optimo hallado es de}\PY{l+s+s2}{\PYZdq{}}\PY{p}{,} \PY{n}{np}\PY{o}{.}\PY{n}{linalg}\PY{o}{.}\PY{n}{norm}\PY{p}{(}\PY{n}{gfun}\PY{p}{(}\PY{n}{xk}\PY{p}{)}\PY{p}{)}\PY{p}{)}
\PY{n+nb}{print}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{El algoritmo termino con condicion de paro: }\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{bl}\PY{p}{)}
\PY{n+nb}{print}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{El valor promedio de los pasos fue: }\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{np}\PY{o}{.}\PY{n}{mean}\PY{p}{(}\PY{n}{rcrd}\PY{p}{[}\PY{p}{:}\PY{p}{,} \PY{l+m+mi}{2}\PY{p}{]}\PY{p}{)}\PY{p}{)}
\PY{n+nb}{print}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{El promedio de iteraciones del algoritmo de backtracking fue de: }\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{np}\PY{o}{.}\PY{n}{mean}\PY{p}{(}\PY{n}{rcrd}\PY{p}{[}\PY{p}{:}\PY{p}{,}\PY{o}{\PYZhy{}}\PY{l+m+mi}{1}\PY{p}{:}\PY{p}{]}\PY{p}{)}\PY{p}{)}

\PY{n}{contornosFnc2D}\PY{p}{(}\PY{n}{ffun}\PY{p}{,} \PY{n}{xsearch}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{]}\PY{p}{,} \PY{n}{xsearch}\PY{p}{[}\PY{l+m+mi}{1}\PY{p}{]}\PY{p}{,} \PY{n}{ysearch}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{]}\PY{p}{,} \PY{n}{ysearch}\PY{p}{[}\PY{l+m+mi}{1}\PY{p}{]}\PY{p}{,} \PY{p}{[}\PY{l+m+mf}{0.5}\PY{p}{,} \PY{l+m+mi}{5}\PY{p}{,} \PY{l+m+mi}{10}\PY{p}{,} \PY{l+m+mi}{25}\PY{p}{,} \PY{l+m+mi}{50}\PY{p}{,} \PY{l+m+mi}{100}\PY{p}{,} \PY{l+m+mi}{150}\PY{p}{,} \PY{l+m+mi}{250}\PY{p}{,} \PY{l+m+mi}{400}\PY{p}{]}\PY{p}{,} \PY{n}{rcrd}\PY{p}{[}\PY{p}{:}\PY{p}{,}\PY{p}{:}\PY{l+m+mi}{2}\PY{p}{]}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

    \begin{Verbatim}[commandchars=\\\{\}]
El numero de iteraciones del algoritmo de descenso fue de:  1797
El punto optimo hallado fue:  [3.00000073 0.50000019]
El valor de la funcion en el punto optimo hallado es:  8.765687729297037e-14
La norma del gradiente en el punto optimo hallado es de 4.7427864732855434e-07
El algoritmo termino con condicion de paro:  True
El valor promedio de los pasos fue:  0.03130671593562186
El promedio de iteraciones del algoritmo de backtracking fue de:
0.07007786429365963
    \end{Verbatim}

    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{Tarea3_files/Tarea3_49_1.png}
    \end{center}
    { \hspace*{\fill} \\}
    
    Funcion de Rosenbrock

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{74}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{n}{ffun} \PY{o}{=} \PY{n}{rosenbrock}
\PY{n}{gfun} \PY{o}{=} \PY{n}{rosenbrock\PYZus{}grad}
\PY{n}{xsearch} \PY{o}{=} \PY{p}{[}\PY{o}{\PYZhy{}}\PY{l+m+mi}{3}\PY{p}{,}\PY{l+m+mi}{3}\PY{p}{]}
\PY{n}{ysearch} \PY{o}{=} \PY{p}{[}\PY{o}{\PYZhy{}}\PY{l+m+mi}{1}\PY{p}{,}\PY{l+m+mi}{5}\PY{p}{]}
\PY{n}{x0} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{array}\PY{p}{(}\PY{p}{[}\PY{o}{\PYZhy{}}\PY{l+m+mf}{2.1}\PY{p}{,} \PY{l+m+mf}{4.5}\PY{p}{]}\PY{p}{)}
\PY{n}{n} \PY{o}{=} \PY{n+nb}{len}\PY{p}{(}\PY{n}{x0}\PY{p}{)}
\PY{n}{tau} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{sqrt}\PY{p}{(}\PY{n}{n}\PY{o}{*}\PY{n}{eps\PYZus{}m}\PY{p}{)}
\PY{n}{a0} \PY{o}{=} \PY{l+m+mf}{1.0}
\PY{n}{rho} \PY{o}{=} \PY{l+m+mf}{0.8}
\PY{n}{c0} \PY{o}{=} \PY{l+m+mf}{0.1}
\PY{n}{NMax} \PY{o}{=} \PY{l+m+mi}{30000}
\PY{n}{KMax} \PY{o}{=} \PY{l+m+mi}{600}
\PY{n}{xk}\PY{p}{,} \PY{n}{kiter}\PY{p}{,} \PY{n}{bl}\PY{p}{,} \PY{n}{rcrd} \PY{o}{=} \PY{n}{descmax\PYZus{}bt\PYZus{}exact}\PY{p}{(}\PY{n}{ffun}\PY{p}{,} \PY{n}{gfun}\PY{p}{,} \PY{n}{x0}\PY{p}{,} \PY{n}{tau}\PY{p}{,} \PY{n}{NMax}\PY{p}{,} \PY{n}{a0}\PY{p}{,} \PY{n}{rho}\PY{p}{,} \PY{n}{c0}\PY{p}{,} \PY{n}{KMax}\PY{p}{)}
\PY{n+nb}{print}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{El numero de iteraciones del algoritmo de descenso fue de: }\PY{l+s+s2}{\PYZdq{}}\PY{p}{,}\PY{n}{kiter}\PY{p}{)}
\PY{n+nb}{print}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{El punto optimo hallado fue: }\PY{l+s+s2}{\PYZdq{}}\PY{p}{,}\PY{n}{xk}\PY{p}{)}
\PY{n+nb}{print}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{El valor de la funcion en el punto optimo hallado es: }\PY{l+s+s2}{\PYZdq{}}\PY{p}{,} \PY{n}{ffun}\PY{p}{(}\PY{n}{xk}\PY{p}{)}\PY{p}{)}
\PY{n+nb}{print}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{La norma del gradiente en el punto optimo hallado es de}\PY{l+s+s2}{\PYZdq{}}\PY{p}{,} \PY{n}{np}\PY{o}{.}\PY{n}{linalg}\PY{o}{.}\PY{n}{norm}\PY{p}{(}\PY{n}{gfun}\PY{p}{(}\PY{n}{xk}\PY{p}{)}\PY{p}{)}\PY{p}{)}
\PY{n+nb}{print}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{El algoritmo termino con condicion de paro: }\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{bl}\PY{p}{)}
\PY{n+nb}{print}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{El valor promedio de los pasos fue: }\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{np}\PY{o}{.}\PY{n}{mean}\PY{p}{(}\PY{n}{rcrd}\PY{p}{[}\PY{p}{:}\PY{p}{,} \PY{l+m+mi}{2}\PY{p}{]}\PY{p}{)}\PY{p}{)}
\PY{n+nb}{print}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{El promedio de iteraciones del algoritmo de backtracking fue de: }\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{np}\PY{o}{.}\PY{n}{mean}\PY{p}{(}\PY{n}{rcrd}\PY{p}{[}\PY{p}{:}\PY{p}{,}\PY{o}{\PYZhy{}}\PY{l+m+mi}{1}\PY{p}{:}\PY{p}{]}\PY{p}{)}\PY{p}{)}

\PY{n}{contornosFnc2D}\PY{p}{(}\PY{n}{ffun}\PY{p}{,} \PY{n}{xsearch}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{]}\PY{p}{,} \PY{n}{xsearch}\PY{p}{[}\PY{l+m+mi}{1}\PY{p}{]}\PY{p}{,} \PY{n}{ysearch}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{]}\PY{p}{,} \PY{n}{ysearch}\PY{p}{[}\PY{l+m+mi}{1}\PY{p}{]}\PY{p}{,} \PY{p}{[}\PY{l+m+mf}{0.5}\PY{p}{,} \PY{l+m+mi}{5}\PY{p}{,} \PY{l+m+mi}{10}\PY{p}{,} \PY{l+m+mi}{25}\PY{p}{,} \PY{l+m+mi}{50}\PY{p}{,} \PY{l+m+mi}{100}\PY{p}{,} \PY{l+m+mi}{150}\PY{p}{,} \PY{l+m+mi}{250}\PY{p}{,} \PY{l+m+mi}{400}\PY{p}{]}\PY{p}{,} \PY{n}{rcrd}\PY{p}{[}\PY{p}{:}\PY{p}{,}\PY{p}{:}\PY{l+m+mi}{2}\PY{p}{]}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

    \begin{Verbatim}[commandchars=\\\{\}]
El numero de iteraciones del algoritmo de descenso fue de:  16942
El punto optimo hallado fue:  [0.99999495 0.99998989]
El valor de la funcion en el punto optimo hallado es:  2.5537215746960312e-11
La norma del gradiente en el punto optimo hallado es de 9.300609259878548e-06
El algoritmo termino con condicion de paro:  True
El valor promedio de los pasos fue:  0.001807405064791999
El promedio de iteraciones del algoritmo de backtracking fue de:
0.005843121052942218
    \end{Verbatim}

    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{Tarea3_files/Tarea3_51_1.png}
    \end{center}
    { \hspace*{\fill} \\}
    
    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{75}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{n}{ffun} \PY{o}{=} \PY{n}{rosenbrock}
\PY{n}{gfun} \PY{o}{=} \PY{n}{rosenbrock\PYZus{}grad}
\PY{n}{xsearch} \PY{o}{=} \PY{p}{[}\PY{o}{\PYZhy{}}\PY{l+m+mi}{3}\PY{p}{,}\PY{l+m+mi}{3}\PY{p}{]}
\PY{n}{ysearch} \PY{o}{=} \PY{p}{[}\PY{o}{\PYZhy{}}\PY{l+m+mi}{1}\PY{p}{,}\PY{l+m+mi}{5}\PY{p}{]}
\PY{n}{x0} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{array}\PY{p}{(}\PY{p}{[}\PY{o}{\PYZhy{}}\PY{l+m+mf}{1.2}\PY{p}{,} \PY{l+m+mf}{1.0}\PY{p}{]}\PY{p}{)}
\PY{n}{n} \PY{o}{=} \PY{n+nb}{len}\PY{p}{(}\PY{n}{x0}\PY{p}{)}
\PY{n}{tau} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{sqrt}\PY{p}{(}\PY{n}{n}\PY{o}{*}\PY{n}{eps\PYZus{}m}\PY{p}{)}
\PY{n}{a0} \PY{o}{=} \PY{l+m+mf}{1.0}
\PY{n}{rho} \PY{o}{=} \PY{l+m+mf}{0.8}
\PY{n}{c0} \PY{o}{=} \PY{l+m+mf}{0.1}
\PY{n}{NMax} \PY{o}{=} \PY{l+m+mi}{30000}
\PY{n}{KMax} \PY{o}{=} \PY{l+m+mi}{600}
\PY{n}{xk}\PY{p}{,} \PY{n}{kiter}\PY{p}{,} \PY{n}{bl}\PY{p}{,} \PY{n}{rcrd} \PY{o}{=} \PY{n}{descmax\PYZus{}bt\PYZus{}exact}\PY{p}{(}\PY{n}{ffun}\PY{p}{,} \PY{n}{gfun}\PY{p}{,} \PY{n}{x0}\PY{p}{,} \PY{n}{tau}\PY{p}{,} \PY{n}{NMax}\PY{p}{,} \PY{n}{a0}\PY{p}{,} \PY{n}{rho}\PY{p}{,} \PY{n}{c0}\PY{p}{,} \PY{n}{KMax}\PY{p}{)}
\PY{n+nb}{print}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{El numero de iteraciones del algoritmo de descenso fue de: }\PY{l+s+s2}{\PYZdq{}}\PY{p}{,}\PY{n}{kiter}\PY{p}{)}
\PY{n+nb}{print}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{El punto optimo hallado fue: }\PY{l+s+s2}{\PYZdq{}}\PY{p}{,}\PY{n}{xk}\PY{p}{)}
\PY{n+nb}{print}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{El valor de la funcion en el punto optimo hallado es: }\PY{l+s+s2}{\PYZdq{}}\PY{p}{,} \PY{n}{ffun}\PY{p}{(}\PY{n}{xk}\PY{p}{)}\PY{p}{)}
\PY{n+nb}{print}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{La norma del gradiente en el punto optimo hallado es de}\PY{l+s+s2}{\PYZdq{}}\PY{p}{,} \PY{n}{np}\PY{o}{.}\PY{n}{linalg}\PY{o}{.}\PY{n}{norm}\PY{p}{(}\PY{n}{gfun}\PY{p}{(}\PY{n}{xk}\PY{p}{)}\PY{p}{)}\PY{p}{)}
\PY{n+nb}{print}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{El algoritmo termino con condicion de paro: }\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{bl}\PY{p}{)}
\PY{n+nb}{print}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{El valor promedio de los pasos fue: }\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{np}\PY{o}{.}\PY{n}{mean}\PY{p}{(}\PY{n}{rcrd}\PY{p}{[}\PY{p}{:}\PY{p}{,} \PY{l+m+mi}{2}\PY{p}{]}\PY{p}{)}\PY{p}{)}
\PY{n+nb}{print}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{El promedio de iteraciones del algoritmo de backtracking fue de: }\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{np}\PY{o}{.}\PY{n}{mean}\PY{p}{(}\PY{n}{rcrd}\PY{p}{[}\PY{p}{:}\PY{p}{,}\PY{o}{\PYZhy{}}\PY{l+m+mi}{1}\PY{p}{:}\PY{p}{]}\PY{p}{)}\PY{p}{)}

\PY{n}{contornosFnc2D}\PY{p}{(}\PY{n}{ffun}\PY{p}{,} \PY{n}{xsearch}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{]}\PY{p}{,} \PY{n}{xsearch}\PY{p}{[}\PY{l+m+mi}{1}\PY{p}{]}\PY{p}{,} \PY{n}{ysearch}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{]}\PY{p}{,} \PY{n}{ysearch}\PY{p}{[}\PY{l+m+mi}{1}\PY{p}{]}\PY{p}{,} \PY{p}{[}\PY{l+m+mf}{0.5}\PY{p}{,} \PY{l+m+mi}{5}\PY{p}{,} \PY{l+m+mi}{10}\PY{p}{,} \PY{l+m+mi}{25}\PY{p}{,} \PY{l+m+mi}{50}\PY{p}{,} \PY{l+m+mi}{100}\PY{p}{,} \PY{l+m+mi}{150}\PY{p}{,} \PY{l+m+mi}{250}\PY{p}{,} \PY{l+m+mi}{400}\PY{p}{]}\PY{p}{,} \PY{n}{rcrd}\PY{p}{[}\PY{p}{:}\PY{p}{,}\PY{p}{:}\PY{l+m+mi}{2}\PY{p}{]}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

    \begin{Verbatim}[commandchars=\\\{\}]
El numero de iteraciones del algoritmo de descenso fue de:  13464
El punto optimo hallado fue:  [0.99999569 0.99999139]
El valor de la funcion en el punto optimo hallado es:  1.8573024113063803e-11
La norma del gradiente en el punto optimo hallado es de 1.2162427761699429e-05
El algoritmo termino con condicion de paro:  True
El valor promedio de los pasos fue:  0.0021307029527849785
El promedio de iteraciones del algoritmo de backtracking fue de:
0.00772372818418121
    \end{Verbatim}

    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{Tarea3_files/Tarea3_52_1.png}
    \end{center}
    { \hspace*{\fill} \\}
    
    \subsubsection{3.- Comentario}\label{comentario}

En el caso de la funcion de Himmelblau el numero de iteraciones del
algoritmo de descenso aumento un poco del paso inexacto al exacto, sin
embargo, hubo una mejora significativa en el promedio de iteraciones del
algorimo de backtracking, respecto a las graficas es posible mencionar
que tienen comportamientos similares.

En la funcion de Beale, mientras que para el primero de los puntos el
numero de iteraciones para el algoritmo de descenso se mantuvo
practicamente igual, en el caso del segundo punto, tuvo una mejora de un
orden de magnitud. De la misma forma, en ambos casos la mejora en el
promedio de iteraciones del algoritmo de backtracking es muy grande.
Respecto a las graficas, mientras que para el primer caso podemos
observar que el comportamiento es similar, el segundo se respalda lo
anterior haciendose visible una mejora.

Por ultimo en la funcion de Rosenbrock, en el caso de las iteraciones
del algoritmo de descenso se tiene una mejora sustancial para el primer
punto, sin embargo, en el segundo punto aunque empeora, es relativamente
poco. Para el promedio de iteraciones del algoritmo de backtracking la
mejora es superior, mientras que las graficas para el caso de paso
exacto se ve que tienen una convergencia lenta, y en el caso inexacto en
el primer punto tambien tiene una convergencia lenta pero en el segundo
caso se aprecia que se queda muy cerca de determinados puntos.

De forma general se puede hablar de que si existe una mejora con el paso
exacto, sobre todo en el algoritmo de backtracking pues en general el
promedio se mantiene muy bajo.

    \subsection{Ejercicio 3}\label{ejercicio-3}

    Sea \(f(x) = f(x_1, x_2) = 5 + x_1^2 + x_2^2\). Si
\(\textbf{x}_0 = (-1, 1)^T\), \(\textbf{p}_0 = (1, 0)^T\) y
\(c_1 = 10^{-4}\). Verifique que \(\textbf{p}_0\) es una direccion de
descenso y encuentre el valor mas grande \(\alpha > 0\) que satisface la
condicion de descenso suficiente.

    Primero validamos que efectivamente \(\textbf{p}_0\) sea una direccion
de descenso. Sea \(\nabla f(x) = (2 x_1, 2 x_2)\) el gradiente de la
funcion entonces verificamos que \(\nabla f(x) \cdot p_0 < 0\)

    \[
\nabla f(x_0) \cdot p_0 = -2 < 0
\]

    Entonces en este caso es una direccion de descenso, ahora berificamos
que sea suficiente tal que

    \[
    f(x_0 + \alpha p_0) \leq f(x_0) + c_1 \alpha p_0^T \nabla f(x_0)
\]

    \[
    f(x_0 + \alpha p_0) = \alpha^2 - 2 \alpha + 7  
\]

    \[
    f(x_0) = 7
\]

    \[
c_1 \alpha p_0^T \nabla f(x_0) = -\frac{2 \alpha}{10^4}
\]

    \[
     \alpha^2 - 2 \alpha + 7  \leq 7 - \frac{2 \alpha}{10^4}
\]

    \[
    \alpha (alpha - 2(1 - 10^{-4})) \leq 0
\]

    dado que \(\alpha >0\), entonces

    \[
    alpha - 2(1 - 10^{-4}) \leq 0
\] \[
    alpha \leq 2(1 - 10^{-4})
\]

    por lo tanto el \(\alpha\) maximo es \[
    \alpha_{max} = 2(1 - 10^{-4})
\]

    \subsection{Ejercicio 4}\label{ejercicio-4}

    Sea \(f:\mathbb{R}^n \to \mathbb{R}\) y \(\textbf{S}\) una matriz no
singular de tamanio \(n \times n\). Si \(\textbf{x} = \textbf{Sy}\) para
\$ \textbf{y} \in \mathbb{R}\^{}n\$ y definimos
\(g(\textbf{y}) = f(\textbf{Sy})\), aplicando la regla de la cadena
muestre que
\[ \nabla g(\textbf{y}) = \textbf{S}^T \nabla f(\textbf{x})\]

    Bien, para poder mostrar lo anterior entonces aplicamos lo siguiente \[
    \frac{\partial g(\textbf{y})}{\partial \textbf{y}} = \frac{\partial f(\textbf{x})}{\partial \textbf{y}}
\] \[
    \frac{\partial f(\textbf{x})}{\partial \textbf{y}} = \frac{\partial f(\textbf{x})}{\partial \textbf{x}} \frac{\partial \textbf{x}}{\partial y}
\] Dado \(\textbf{x} = \textbf{Sy}\), entonces

    \[
    \frac{\partial g(\textbf{y})}{\partial \textbf{y}} = \textbf{S}^T \nabla f(\textbf{x})
\]

    Entonces aplicando el metodo del maximo descenso a la funcion \(g\) es:
\[
    y_{k+1} = y_k - \alpha_k \textbf{S}^T \nabla f(\textbf{Sy}_k)
\] Multiplicando \(\textbf{S}\) ambos miembros de la ecuacion y
utilizando la notacion \(\textbf{x}_x = \textbf{Sy}_k\): \[
    \textbf{x}_{k+1} = \textbf{Sy}_k - \alpha_k \textbf{SS}^T \nabla f(\textbf{Sy}_k)
\] Si \(\textbf{D} = \textbf{SS}^T\), se obtiene el metodo del gradiente
escalado: \[
    \textbf{x}_{k+1} = \textbf{x}_k - \alpha_k \textbf{D} \nabla f(\textbf{x}_k)
\] Muestre que \(- \textbf{D} \nabla f(\textbf{x}_k)\) es una direccion
de descenso.

    Bien entonces debemos verificar que el producto escalar entre el
gradiente de la funcion y la direccion sea negativo para verificar que
este sea una direccion de descenso, entonces \[
    - \textbf{D} \nabla f(\textbf{x}_k) \cdot \nabla f(\textbf{x}_k) < 0
\]

    Entonces, esto se reescribe tal que \[
    - \textbf{D} || \nabla f(\textbf{x}) ||^2
\] Derivado del hecho de que \(\textbf{S}\) es una matriz no singular,
entonces se tiene que \(\textbf{D} = \textbf{SS}^T\) es una matriz
simetrica definida positiva, por ende, este valor cumple con ser una
direccion de descenso.


    % Add a bibliography block to the postdoc
    
    
    
\end{document}
